{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":116062,"databundleVersionId":14084779,"sourceType":"competition"}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom Bio import SeqIO\nimport torch\nimport numpy as np\nfrom transformers import AutoTokenizer, AutoModel, T5Tokenizer, T5EncoderModel\nfrom tqdm import tqdm\nimport logging\nfrom pathlib import Path\nimport re\nimport os, json, math, time, sys\nfrom typing import Dict, Tuple, List\nimport scipy.sparse as sp\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score, precision_recall_curve\nfrom tqdm import tqdm\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\nimport torch.serialization as _ser\n_ser.add_safe_globals([np.core.multiarray.scalar])\n\nstate = torch.load(last_ckpt, map_location=DEVICE, weights_only=False)\n\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(\"Device =\", DEVICE)\n# Load ProtT5 XL UniRef50 Model + Tokenizer\ntokenizer = T5Tokenizer.from_pretrained(\"Rostlab/prot_t5_xl_half_uniref50-enc\", do_lower_case=False)\nmodel = T5EncoderModel.from_pretrained(\"Rostlab/prot_t5_xl_half_uniref50-enc\")\nmodel = model.to(DEVICE)\nmodel.eval()\n\nprint(\"[OK] ProtT5 model + tokenizer loaded.\")\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\nprint(\"Using device:\", DEVICE)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-30T09:12:54.006034Z","iopub.execute_input":"2025-11-30T09:12:54.006685Z","iopub.status.idle":"2025-11-30T09:12:58.647771Z","shell.execute_reply.started":"2025-11-30T09:12:54.006645Z","shell.execute_reply":"2025-11-30T09:12:58.646907Z"}},"outputs":[{"name":"stdout","text":"Device = cuda\n[OK] ProtT5 model + tokenizer loaded.\nUsing device: cuda\n/kaggle/input/cafa-6-protein-function-prediction/sample_submission.tsv\n/kaggle/input/cafa-6-protein-function-prediction/IA.tsv\n/kaggle/input/cafa-6-protein-function-prediction/Test/testsuperset.fasta\n/kaggle/input/cafa-6-protein-function-prediction/Test/testsuperset-taxon-list.tsv\n/kaggle/input/cafa-6-protein-function-prediction/Train/train_terms.tsv\n/kaggle/input/cafa-6-protein-function-prediction/Train/train_sequences.fasta\n/kaggle/input/cafa-6-protein-function-prediction/Train/train_taxonomy.tsv\n/kaggle/input/cafa-6-protein-function-prediction/Train/go-basic.obo\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"import torch, transformers, google.protobuf\nprint(\"torch.cuda:\", torch.cuda.is_available(), torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"no-gpu\")\nprint(\"torch.__version__:\", torch.__version__)\nprint(\"transformers:\", transformers.__version__)\nprint(\"protobuf:\", google.protobuf.__version__)\n\n# quick import check to ensure no JAX/XLA interference:\ntry:\n    import jax\n    print(\"JAX detected:\", jax.__version__)\nexcept Exception as e:\n    print(\"JAX not present or import failed (OK if you don't use TPU):\", e)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T09:13:07.761972Z","iopub.execute_input":"2025-11-30T09:13:07.762271Z","iopub.status.idle":"2025-11-30T09:13:07.768347Z","shell.execute_reply.started":"2025-11-30T09:13:07.762247Z","shell.execute_reply":"2025-11-30T09:13:07.767304Z"}},"outputs":[{"name":"stdout","text":"torch.cuda: True Tesla T4\ntorch.__version__: 2.6.0+cu124\ntransformers: 4.53.3\nprotobuf: 6.33.0\nJAX detected: 0.5.2\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(message)s\")\nlogger = logging.getLogger(__name__)\n\n# ===========================================================\n# FORCE GPU\n# ===========================================================\nassert torch.cuda.is_available(), \"âŒ GPU NOT AVAILABLE â€” KAGGLE RUNNING ON CPU!\"\n\nDEVICE = torch.device(\"cuda\")\nlogger.info(f\"ðŸ”¥ GPU ACTIVE: {torch.cuda.get_device_name(0)}\")\n\n\n# ===========================================================\n# FASTA LOADER\n# ===========================================================\ndef load_fasta(path):\n    ids, seqs = [], []\n    with open(path, \"r\") as f:\n        cur = None\n        buf = []\n        for line in f:\n            line = line.strip()\n            if line.startswith(\">\"):\n                if cur:\n                    seqs.append(\"\".join(buf))\n                cur = line[1:]\n                ids.append(cur)\n                buf = []\n            else:\n                buf.append(line)\n        seqs.append(\"\".join(buf))\n    return ids, seqs\n\n\n\n# ===========================================================\n# GPU-FORCED PROTBERT EMBEDDER\n# ===========================================================\nclass ProtBertEmbedder:\n    def __init__(self, model_name=\"Rostlab/prot_bert_bfd\"):\n\n        logger.info(f\"Loading ProtBERT from HuggingFace â†’ GPU\")\n        self.tokenizer = AutoTokenizer.from_pretrained(model_name, do_lower_case=False)\n        self.model = AutoModel.from_pretrained(model_name)\n        self.model = self.model.to(DEVICE)\n        self.model.eval()\n\n        logger.info(\"Model successfully moved to GPU âœ”\")\n\n    def embed(self, sequences, batch_size=16):\n        logger.info(f\"Embedding {len(sequences)} sequences using GPU\")\n\n        seqs = [re.sub(\"[UZOB]\", \"X\", s) for s in sequences]\n        embeddings = []\n\n        # GPU autocast for speed + memory reduction\n        autocast = torch.cuda.amp.autocast\n\n        with torch.inference_mode():\n            for i in tqdm(range(0, len(seqs), batch_size), desc=\"GPU-ProtBERT\"):\n\n                batch = seqs[i : i + batch_size]\n\n                # --- TOKENIZATION (CPU â†’ GPU explicitly) ---\n                toks = self.tokenizer(\n                    batch,\n                    add_special_tokens=True,\n                    padding=True,\n                    truncation=True,\n                    return_tensors=\"pt\"\n                )\n\n                # Force token tensors to GPU\n                toks = {k: v.to(DEVICE) for k, v in toks.items()}\n\n                # --- MODEL FORWARD PASS ON GPU ---\n                with autocast(dtype=torch.float16):\n                    out = self.model(**toks)\n\n                hidden = out.last_hidden_state     # (B, L, 1024)\n                mask = toks[\"attention_mask\"]\n\n                # --- MEAN POOLING ---\n                for j in range(hidden.size(0)):\n                    L = mask[j].sum()\n                    emb = hidden[j, :L].mean(dim=0)\n                    embeddings.append(emb.float().cpu().numpy())\n\n        return np.vstack(embeddings)\n\n\n\n# ===========================================================\n# MAIN SCRIPT\n# ===========================================================\nTRAIN_FASTA = Path(\"/kaggle/input/cafa-6-protein-function-prediction/Train/train_sequences.fasta\")\nTEST_FASTA  = Path(\"/kaggle/input/cafa-6-protein-function-prediction/Test/testsuperset.fasta\")\n\nOUTPUT = Path(\"/kaggle/working/embeddings\")\nOUTPUT.mkdir(exist_ok=True)\n\nlogger.info(\"Loading FASTA...\")\n\ntrain_ids, train_seqs = load_fasta(TRAIN_FASTA)\ntest_ids, test_seqs   = load_fasta(TEST_FASTA)\n\ntrain_sequences = dict(zip(train_ids, train_seqs))\ntest_sequences = dict(zip(test_ids, test_seqs))\n\nembedder = ProtBertEmbedder()\n\n# ========================= TRAIN =============================\nlogger.info(\"ðŸ”¥ Using GPU to embed TRAIN sequences...\")\ntrain_embs = []\nBATCH = 16\n\nfor i in range(0, len(train_ids), BATCH):\n    batch_ids = train_ids[i : i + BATCH]\n    batch_seqs = [train_sequences[x] for x in batch_ids]\n\n    batch_embs = embedder.embed(batch_seqs, batch_size=BATCH)\n    train_embs.append(batch_embs)\n\ntrain_embs = np.vstack(train_embs)\nnp.save(OUTPUT / \"train_protbert_gpu.npy\", train_embs)\nlogger.info(f\"[OK] Saved TRAIN embeddings: {train_embs.shape}\")\n\n\n# ========================= TEST ==============================\nlogger.info(\"ðŸ”¥ Using GPU to embed TEST sequences...\")\n\ntest_embs = []\n\nfor i in range(0, len(test_ids), BATCH):\n    batch_ids = test_ids[i : i + BATCH]\n    batch_seqs = [test_sequences[x] for x in batch_ids]\n\n    batch_embs = embedder.embed(batch_seqs, batch_size=BATCH)\n    test_embs.append(batch_embs)\n\ntest_embs = np.vstack(test_embs)\nnp.save(OUTPUT / \"test_protbert_gpu.npy\", test_embs)\nlogger.info(f\"[OK] Saved TEST embeddings: {test_embs.shape}\")\n\nlogger.info(\"=== GPU EMBEDDING COMPLETE ===\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T09:13:19.218928Z","iopub.execute_input":"2025-11-30T09:13:19.219239Z","iopub.status.idle":"2025-11-30T09:13:28.201155Z","shell.execute_reply.started":"2025-11-30T09:13:19.219201Z","shell.execute_reply":"2025-11-30T09:13:28.199796Z"}},"outputs":[{"name":"stderr","text":"GPU-ProtBERT:   0%|          | 0/1 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n/tmp/ipykernel_47/2648483844.py:77: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with autocast(dtype=torch.float16):\nGPU-ProtBERT: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 28.56it/s]\nGPU-ProtBERT: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 31.18it/s]\nGPU-ProtBERT: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 35.28it/s]\nGPU-ProtBERT: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 31.82it/s]\nGPU-ProtBERT: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 32.85it/s]\nGPU-ProtBERT: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 33.44it/s]\nGPU-ProtBERT: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 33.37it/s]\nGPU-ProtBERT: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 31.02it/s]\nGPU-ProtBERT: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 32.41it/s]\nGPU-ProtBERT: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 30.87it/s]\nGPU-ProtBERT: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 32.58it/s]\nGPU-ProtBERT: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 32.45it/s]\nGPU-ProtBERT: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 31.04it/s]\nGPU-ProtBERT: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 32.17it/s]\nGPU-ProtBERT: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 33.09it/s]\nGPU-ProtBERT: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 31.69it/s]\nGPU-ProtBERT: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 31.87it/s]\nGPU-ProtBERT: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 32.85it/s]\nGPU-ProtBERT: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 33.47it/s]\nGPU-ProtBERT: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 32.05it/s]\nGPU-ProtBERT: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 32.69it/s]\nGPU-ProtBERT: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 31.97it/s]\nGPU-ProtBERT: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 33.82it/s]\nGPU-ProtBERT: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 34.46it/s]\nGPU-ProtBERT: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 35.02it/s]\nGPU-ProtBERT: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 33.72it/s]\nGPU-ProtBERT: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 34.60it/s]\nGPU-ProtBERT: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 34.61it/s]\nGPU-ProtBERT: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 33.36it/s]\nGPU-ProtBERT: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 32.95it/s]\nGPU-ProtBERT: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 33.00it/s]\nGPU-ProtBERT: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 34.99it/s]\nGPU-ProtBERT: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 33.31it/s]\nGPU-ProtBERT: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 33.80it/s]\nGPU-ProtBERT: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 35.01it/s]\nGPU-ProtBERT: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 35.53it/s]\nGPU-ProtBERT: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 33.00it/s]\nGPU-ProtBERT: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 34.55it/s]\nGPU-ProtBERT: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 25.09it/s]\nGPU-ProtBERT: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 33.61it/s]\nGPU-ProtBERT: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 31.60it/s]\nGPU-ProtBERT: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 26.11it/s]\nGPU-ProtBERT: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 32.22it/s]\nGPU-ProtBERT: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 34.81it/s]\nGPU-ProtBERT: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 33.15it/s]\nGPU-ProtBERT: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 32.55it/s]\nGPU-ProtBERT: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 33.35it/s]\nGPU-ProtBERT: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 28.90it/s]\nGPU-ProtBERT: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 30.78it/s]\nGPU-ProtBERT: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 33.58it/s]\nGPU-ProtBERT: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 34.89it/s]\nGPU-ProtBERT: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 31.99it/s]\nGPU-ProtBERT: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 33.60it/s]\nGPU-ProtBERT: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 33.96it/s]\nGPU-ProtBERT: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 33.09it/s]\nGPU-ProtBERT: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 32.50it/s]\nGPU-ProtBERT: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 32.68it/s]\nGPU-ProtBERT: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 31.98it/s]\nGPU-ProtBERT: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 32.86it/s]\nGPU-ProtBERT: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 31.51it/s]\nGPU-ProtBERT: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 32.67it/s]\nGPU-ProtBERT: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 31.92it/s]\nGPU-ProtBERT: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 33.25it/s]\nGPU-ProtBERT: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 32.51it/s]\nGPU-ProtBERT: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 32.80it/s]\nGPU-ProtBERT: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 27.70it/s]\nGPU-ProtBERT: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 27.57it/s]\nGPU-ProtBERT: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 31.75it/s]\nGPU-ProtBERT: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 31.64it/s]\nGPU-ProtBERT: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 31.35it/s]\nGPU-ProtBERT: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 31.94it/s]\nGPU-ProtBERT: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 24.58it/s]\nGPU-ProtBERT: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 32.16it/s]\nGPU-ProtBERT: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 30.93it/s]\nGPU-ProtBERT: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 30.50it/s]\nGPU-ProtBERT: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 31.06it/s]\nGPU-ProtBERT: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 25.92it/s]\nGPU-ProtBERT: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 30.69it/s]\nGPU-ProtBERT: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 31.74it/s]\nGPU-ProtBERT: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 29.78it/s]\nGPU-ProtBERT: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 32.49it/s]\nGPU-ProtBERT: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 33.16it/s]\nGPU-ProtBERT: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 32.16it/s]\nGPU-ProtBERT: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 30.12it/s]\nGPU-ProtBERT: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 32.95it/s]\nGPU-ProtBERT: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 31.89it/s]\nGPU-ProtBERT: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 30.80it/s]\nGPU-ProtBERT: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 31.20it/s]\nGPU-ProtBERT: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 30.43it/s]\nGPU-ProtBERT: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 30.85it/s]\nGPU-ProtBERT: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 29.27it/s]\nGPU-ProtBERT: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 28.11it/s]\nGPU-ProtBERT: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 32.12it/s]\nGPU-ProtBERT: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 32.21it/s]\nGPU-ProtBERT: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 32.08it/s]\nGPU-ProtBERT: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 33.49it/s]\nGPU-ProtBERT: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 32.16it/s]\nGPU-ProtBERT: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 31.51it/s]\nGPU-ProtBERT: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 32.61it/s]\nGPU-ProtBERT: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 34.18it/s]\nGPU-ProtBERT: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 30.32it/s]\nGPU-ProtBERT: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 32.98it/s]\nGPU-ProtBERT: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 31.93it/s]\nGPU-ProtBERT: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 33.40it/s]\nGPU-ProtBERT: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 32.63it/s]\nGPU-ProtBERT: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 31.83it/s]\nGPU-ProtBERT: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 32.32it/s]\nGPU-ProtBERT: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 33.12it/s]\nGPU-ProtBERT: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 30.36it/s]\nGPU-ProtBERT: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 34.45it/s]\nGPU-ProtBERT: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 33.51it/s]\nGPU-ProtBERT: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 33.59it/s]\nGPU-ProtBERT: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 33.03it/s]\nGPU-ProtBERT: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 32.93it/s]\nGPU-ProtBERT: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 32.95it/s]\nGPU-ProtBERT: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 34.40it/s]\nGPU-ProtBERT: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 33.51it/s]\nGPU-ProtBERT: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 33.77it/s]\nGPU-ProtBERT: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 33.25it/s]\nGPU-ProtBERT: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 27.41it/s]\nGPU-ProtBERT: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 33.71it/s]\nGPU-ProtBERT: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 33.13it/s]\nGPU-ProtBERT: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 33.27it/s]\nGPU-ProtBERT: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 33.56it/s]\nGPU-ProtBERT: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 33.37it/s]\nGPU-ProtBERT: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 32.57it/s]\nGPU-ProtBERT: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 32.76it/s]\nGPU-ProtBERT: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 32.74it/s]\nGPU-ProtBERT: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 32.75it/s]\nGPU-ProtBERT: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 31.54it/s]\nGPU-ProtBERT: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 31.47it/s]\nGPU-ProtBERT: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 32.03it/s]\nGPU-ProtBERT: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 31.90it/s]\nGPU-ProtBERT: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 28.28it/s]\nGPU-ProtBERT: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 35.22it/s]\nGPU-ProtBERT: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 33.59it/s]\nGPU-ProtBERT:   0%|          | 0/1 [00:00<?, ?it/s]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_47/2648483844.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    119\u001b[0m     \u001b[0mbatch_seqs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtrain_sequences\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch_ids\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m     \u001b[0mbatch_embs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membedder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_seqs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBATCH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m     \u001b[0mtrain_embs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_embs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_47/2648483844.py\u001b[0m in \u001b[0;36membed\u001b[0;34m(self, sequences, batch_size)\u001b[0m\n\u001b[1;32m     76\u001b[0m                 \u001b[0;31m# --- MODEL FORWARD PASS ON GPU ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mautocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m                     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mtoks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m                 \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_hidden_state\u001b[0m     \u001b[0;31m# (B, L, 1024)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    994\u001b[0m         \u001b[0mhead_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_head_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_hidden_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    995\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 996\u001b[0;31m         encoder_outputs = self.encoder(\n\u001b[0m\u001b[1;32m    997\u001b[0m             \u001b[0membedding_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    998\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    649\u001b[0m             \u001b[0mpast_key_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpast_key_values\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpast_key_values\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    650\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 651\u001b[0;31m             layer_outputs = layer_module(\n\u001b[0m\u001b[1;32m    652\u001b[0m                 \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    653\u001b[0m                 \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_layers.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gradient_checkpointing_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    551\u001b[0m         \u001b[0;31m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m         \u001b[0mself_attn_past_key_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpast_key_value\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpast_key_value\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 553\u001b[0;31m         self_attention_outputs = self.attention(\n\u001b[0m\u001b[1;32m    554\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    481\u001b[0m         \u001b[0moutput_attentions\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    482\u001b[0m     ) -> tuple[torch.Tensor]:\n\u001b[0;32m--> 483\u001b[0;31m         self_outputs = self.self(\n\u001b[0m\u001b[1;32m    484\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    485\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    374\u001b[0m             \u001b[0mkey_layer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpast_key_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 376\u001b[0;31m             \u001b[0mkey_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose_for_scores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    377\u001b[0m             \u001b[0mvalue_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose_for_scores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpast_key_value\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_cross_attention\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mtranspose_for_scores\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    219\u001b[0m         \u001b[0mnew_x_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_attention_heads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention_head_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_x_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     def forward(\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":23},{"cell_type":"code","source":"wrong_df = pd.read_csv(\"/kaggle/input/cafa-6-protein-function-prediction/Train/train_terms.tsv\", sep=\"\\t\", header=None)\nprint(wrong_df.head(20))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T09:13:42.670113Z","iopub.execute_input":"2025-11-30T09:13:42.670404Z","iopub.status.idle":"2025-11-30T09:13:42.906739Z","shell.execute_reply.started":"2025-11-30T09:13:42.670378Z","shell.execute_reply":"2025-11-30T09:13:42.905887Z"}},"outputs":[{"name":"stdout","text":"          0           1       2\n0   EntryID        term  aspect\n1    Q5W0B1  GO:0000785       C\n2    Q5W0B1  GO:0004842       F\n3    Q5W0B1  GO:0051865       P\n4    Q5W0B1  GO:0006275       P\n5    Q5W0B1  GO:0006513       P\n6    Q5W0B1  GO:0003682       F\n7    Q5W0B1  GO:0005515       F\n8    Q3EC77  GO:0000138       C\n9    Q3EC77  GO:0005794       C\n10   Q8IZR5  GO:0005515       F\n11   Q8R2Z3  GO:0140900       F\n12   Q8R2Z3  GO:0005254       F\n13   Q8R2Z3  GO:0035429       P\n14   Q8R2Z3  GO:0016323       C\n15   Q8R2Z3  GO:1902358       P\n16   Q8R2Z3  GO:0015701       P\n17   Q8R2Z3  GO:0015705       P\n18   Q8R2Z3  GO:0015706       P\n19   Q8R2Z3  GO:0019532       P\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"# 02_build_go_label_matrix.py\nimport os\nimport json\nfrom pathlib import Path\nfrom collections import defaultdict, deque\nfrom scipy.sparse import csr_matrix, save_npz\n\n# ---------- CONFIG ----------\nBASE = Path(\"/kaggle/working\")               # change if local\nDATA_DIR = Path(\"/kaggle/input/cafa-6-protein-function-prediction/\")\nTRAIN_TERMS = DATA_DIR / \"Train\" / \"train_terms.tsv\"\nGO_OBO = DATA_DIR / \"Train\" / \"go-basic.obo\"\nOUT = BASE / \"outputs_labels\"\nOUT.mkdir(parents=True, exist_ok=True)\n\n# ---------- HELPERS ----------\ndef parse_obo(path):\n    \"\"\"\n    Parse go-basic.obo to extract:\n        term -> {'id': GO:xxxx, 'namespace': MF/BP/CC, 'is_a': [parents]}\n    Returns dict of id -> node.\n    \"\"\"\n    nodes = {}\n    cur = None\n    for line in open(path, \"r\", encoding=\"utf-8\"):\n        line = line.strip()\n        if line == \"[Term]\":\n            cur = {}\n            continue\n        if line == \"\" and cur and \"id\" in cur:\n            nodes[cur[\"id\"]] = cur\n            cur = None\n            continue\n        if cur is None:\n            continue\n        if line.startswith(\"id: \"):\n            cur[\"id\"] = line.split(\"id: \")[1].strip()\n        elif line.startswith(\"namespace: \"):\n            cur[\"namespace\"] = line.split(\"namespace: \")[1].strip()\n        elif line.startswith(\"is_a: \"):\n            parent = line.split(\"is_a: \")[1].split(\" ! \")[0].strip()\n            cur.setdefault(\"is_a\", []).append(parent)\n    # last term\n    if cur and \"id\" in cur:\n        nodes[cur[\"id\"]] = cur\n    return nodes\n\ndef build_ancestors(nodes):\n    \"\"\"\n    Build ancestor sets for every term via BFS up the DAG.\n    Returns dict term -> set(ancestors incl. self).\n    \"\"\"\n    ancestors = {}\n    # prefill missing parents\n    for tid, node in nodes.items():\n        node.setdefault(\"is_a\", [])\n    for tid in nodes:\n        # BFS/DFS up\n        stack = [tid]\n        seen = set()\n        while stack:\n            t = stack.pop()\n            if t in seen:\n                continue\n            seen.add(t)\n            parents = nodes.get(t, {}).get(\"is_a\", [])\n            for p in parents:\n                if p and p not in seen:\n                    stack.append(p)\n        ancestors[tid] = seen  # includes self\n    return ancestors\n\n# ---------- LOAD TRAIN TERMS ----------\nprint(\"[*] Loading train terms...\")\ntrain_df = pd.read_csv(TRAIN_TERMS, sep=\"\\t\")\ntrain_df.columns = [\"protein\", \"go\", \"aspect\"]\n\nprint(f\"[*] Loaded {len(train_df)} annotations; unique proteins: {train_df['protein'].nunique()}\")\n\n# ---------- LOAD GO OBO ----------\nprint(\"[*] Parsing GO OBO...\")\nnodes = parse_obo(str(GO_OBO))\nprint(f\"[*] Parsed {len(nodes)} GO nodes from OBO\")\n\nanc = build_ancestors(nodes)\nprint(\"[*] Built ancestors for GO terms\")\n\n# ---------- Filter GO terms present in OBO ----------\nall_terms_in_train = set(train_df[\"go\"].unique())\npresent_terms = sorted(t for t in all_terms_in_train if t in nodes)\nmissing_terms = sorted(list(all_terms_in_train - set(present_terms)))\nprint(f\"[*] Terms in train: {len(all_terms_in_train)}; present in OBO: {len(present_terms)}; missing: {len(missing_terms)}\")\nif missing_terms:\n    print(\"Sample missing terms:\", missing_terms[:10])\n\n# ---------- Partition terms by namespace ----------\nnamespace_terms = defaultdict(list)\nfor t in present_terms:\n    ns = nodes[t].get(\"namespace\", \"unknown\")\n    namespace_terms[ns].append(t)\n\nfor ns in [\"molecular_function\", \"biological_process\", \"cellular_component\"]:\n    print(f\"[*] {ns}: {len(namespace_terms.get(ns,[]))} terms\")\n\n# ---------- Build term -> index maps per subontology ----------\nmaps = {}\nfor ns in [\"molecular_function\", \"biological_process\", \"cellular_component\"]:\n    terms = sorted(namespace_terms.get(ns, []))\n    maps[ns] = {\"terms\": terms, \"index\": {t: i for i, t in enumerate(terms)}}\n    print(f\"[*] {ns} indices: {len(terms)}\")\n\n# ---------- Build protein list and mapping ----------\nproteins = sorted(train_df[\"protein\"].unique())\nprotein_index = {p: i for i, p in enumerate(proteins)}\nprint(f\"[*] Proteins count: {len(proteins)}\")\n\n# ---------- Build sparse matrices ----------\ndef build_sparse_for_ns(ns):\n    terms = maps[ns][\"terms\"]\n    term_idx = maps[ns][\"index\"]\n    rows = []\n    cols = []\n    data = []\n    for prot, group in train_df.groupby(\"protein\"):\n        pid = protein_index[prot]\n        gos = set(group[\"go\"].tolist())\n        # keep only gos that belong to this namespace and are present\n        gos = {g for g in gos if g in term_idx}\n        if not gos:\n            continue\n        # expand to ancestors, filter to namespace\n        expanded = set()\n        for g in gos:\n            expanded |= anc.get(g, set())\n        expanded = {e for e in expanded if e in term_idx}\n        for g in expanded:\n            rows.append(pid)\n            cols.append(term_idx[g])\n            data.append(1)\n    if not rows:\n        print(f\"[!] No annotations for {ns}\")\n        return None\n    mat = csr_matrix((data, (rows, cols)), shape=(len(proteins), len(terms)), dtype=np.uint8)\n    return mat\n\n# build and save per-namespace\nfor ns_label, out_name in [(\"molecular_function\", \"labels_MF.npz\"),\n                           (\"biological_process\", \"labels_BP.npz\"),\n                           (\"cellular_component\", \"labels_CC.npz\")]:\n    print(f\"[*] Building matrix for {ns_label} ...\")\n    mat = build_sparse_for_ns(ns_label)\n    if mat is None:\n        print(f\"[!] {ns_label} empty, skipping save.\")\n        continue\n    save_path = OUT / out_name\n    save_npz(save_path, mat)\n    print(f\"[OK] Saved {ns_label} matrix -> {save_path}, shape = {mat.shape}, nnz={mat.nnz}\")\n\n# ---------- Save auxiliary maps ----------\nmaps_out = {\n    \"protein_index\": {p: i for p, i in protein_index.items()},\n    \"mf_terms\": maps[\"molecular_function\"][\"terms\"],\n    \"bp_terms\": maps[\"biological_process\"][\"terms\"],\n    \"cc_terms\": maps[\"cellular_component\"][\"terms\"]\n}\nwith open(OUT / \"maps.json\", \"w\") as fh:\n    json.dump(maps_out, fh)\n\nprint(\"[*] All done. Label matrices saved to:\", OUT)\n\nanc = build_ancestors(nodes)\nprint(\"[*] Built ancestors for GO terms\")\n# Save ancestors for inference\nwith open(OUT / \"ancestors.json\", \"w\") as f:\n    json.dump({k: list(v) for k, v in anc.items()}, f)\nprint(\"[OK] Saved ancestors.json\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T10:26:51.358340Z","iopub.execute_input":"2025-11-30T10:26:51.358587Z","iopub.status.idle":"2025-11-30T10:27:04.555957Z","shell.execute_reply.started":"2025-11-30T10:26:51.358568Z","shell.execute_reply":"2025-11-30T10:27:04.555130Z"}},"outputs":[{"name":"stdout","text":"[*] Loading train terms...\n[*] Loaded 537027 annotations; unique proteins: 82404\n[*] Parsing GO OBO...\n[*] Parsed 48101 GO nodes from OBO\n[*] Built ancestors for GO terms\n[*] Terms in train: 26125; present in OBO: 26125; missing: 0\n[*] molecular_function: 6616 terms\n[*] biological_process: 16858 terms\n[*] cellular_component: 2651 terms\n[*] molecular_function indices: 6616\n[*] biological_process indices: 16858\n[*] cellular_component indices: 2651\n[*] Proteins count: 82404\n[*] Building matrix for molecular_function ...\n[OK] Saved molecular_function matrix -> /kaggle/working/outputs_labels/labels_MF.npz, shape = (82404, 6616), nnz=427243\n[*] Building matrix for biological_process ...\n[OK] Saved biological_process matrix -> /kaggle/working/outputs_labels/labels_BP.npz, shape = (82404, 16858), nnz=1270993\n[*] Building matrix for cellular_component ...\n[OK] Saved cellular_component matrix -> /kaggle/working/outputs_labels/labels_CC.npz, shape = (82404, 2651), nnz=403670\n[*] All done. Label matrices saved to: /kaggle/working/outputs_labels\n[*] Built ancestors for GO terms\n[OK] Saved ancestors.json\n","output_type":"stream"}],"execution_count":41},{"cell_type":"code","source":"import torch.serialization as _ser\n\n_ser.add_safe_globals([\n    np.dtype,\n    np.core.multiarray.scalar\n])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T09:45:02.760365Z","iopub.execute_input":"2025-11-30T09:45:02.761076Z","iopub.status.idle":"2025-11-30T09:45:02.765417Z","shell.execute_reply.started":"2025-11-30T09:45:02.761045Z","shell.execute_reply":"2025-11-30T09:45:02.764643Z"}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"state = torch.load(last_ckpt, map_location=DEVICE, weights_only=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T09:45:04.934704Z","iopub.execute_input":"2025-11-30T09:45:04.935297Z","iopub.status.idle":"2025-11-30T09:45:05.064778Z","shell.execute_reply.started":"2025-11-30T09:45:04.935274Z","shell.execute_reply":"2025-11-30T09:45:05.063982Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"# ------------------------\n# CONFIG\n# ------------------------\nBASE = Path(\"/kaggle/working\")\nEMB_PATH = BASE / \"embeddings\" / \"train_protbert_gpu.npy\"\nLABEL_DIR = BASE / \"outputs_labels\"\nMODEL_DIR = BASE / \"models\"\nMODEL_DIR.mkdir(exist_ok=True)\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nSEED = 42\n\n# training hyperparams - adjust if you need\nBATCH_SIZE = 128         # increase/decrease per GPU memory\nEPOCHS = 10\nLR = 1e-4\nWEIGHT_DECAY = 1e-5\nPATIENCE = 8             # early stopping on val F1 (mean of three heads)\nPROJ_DIM = 512           # backbone projection dim\nHID = 1024\nDROPOUT = 0.3\nNUM_WORKERS = 4\n\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\n\nprint(\"DEVICE:\", DEVICE)\n# ------------------------\n# HELPERS: load embeddings + labels\n# ------------------------\ndef load_embeddings(path: Path) -> np.ndarray:\n    print(\"Loading embeddings (memmap)...\", path)\n    arr = np.load(path, mmap_mode=\"r\")\n    print(\"Embeddings shape:\", arr.shape)\n    return arr\n\ndef load_label_npz(path: Path) -> sp.csr_matrix:\n    print(\"Loading label npz:\", path)\n    return sp.load_npz(str(path)).tocsr()\n\n# ------------------------\n# Dataset\n# ------------------------\nclass EmbLabelDataset(Dataset):\n    def __init__(self, emb_mem: np.ndarray, label_matrices: Dict[str, sp.csr_matrix], indices: List[int]):\n        \"\"\"\n        emb_mem: memory-mapped numpy array (n_proteins, emb_dim)\n        label_matrices: dict of 'mf','bp','cc' -> csr_matrix (n_proteins x n_terms)\n        indices: list of integer protein indices to include (train/val split)\n        \"\"\"\n        self.emb = emb_mem\n        self.labels = label_matrices\n        self.indices = indices\n\n    def __len__(self):\n        return len(self.indices)\n\n    def __getitem__(self, idx):\n        i = self.indices[idx]\n        x = np.array(self.emb[i], dtype=np.float32)           # (D,)\n        # get dense per-ontology label vectors (convert sparse row -> dense)\n        y_mf = self.labels['mf'][i].toarray().ravel().astype(np.uint8) if self.labels['mf'].shape[0] > 0 else np.zeros(0, dtype=np.uint8)\n        y_bp = self.labels['bp'][i].toarray().ravel().astype(np.uint8) if self.labels['bp'].shape[0] > 0 else np.zeros(0, dtype=np.uint8)\n        y_cc = self.labels['cc'][i].toarray().ravel().astype(np.uint8) if self.labels['cc'].shape[0] > 0 else np.zeros(0, dtype=np.uint8)\n        return x, y_mf, y_bp, y_cc\n\ndef collate_fn(batch):\n    X = torch.tensor(np.stack([b[0] for b in batch]), dtype=torch.float32)\n    Ym = [b[1] for b in batch]\n    Yb = [b[2] for b in batch]\n    Yc = [b[3] for b in batch]\n    # Convert lists of arrays -> tensors with appropriate shapes\n    Ym = torch.tensor(np.vstack(Ym), dtype=torch.float32) if Ym[0].size else torch.empty((len(batch),0), dtype=torch.float32)\n    Yb = torch.tensor(np.vstack(Yb), dtype=torch.float32) if Yb[0].size else torch.empty((len(batch),0), dtype=torch.float32)\n    Yc = torch.tensor(np.vstack(Yc), dtype=torch.float32) if Yc[0].size else torch.empty((len(batch),0), dtype=torch.float32)\n    return X, Ym, Yb, Yc\n\n# ------------------------\n# Model\n# ------------------------\nclass MultiHeadNet(nn.Module):\n    def __init__(self, emb_dim:int, proj_dim:int, hid:int, n_mf:int, n_bp:int, n_cc:int, dropout=float(0.3)):\n        super().__init__()\n        # shared projection\n        self.proj = nn.Sequential(\n            nn.LayerNorm(emb_dim),\n            nn.Linear(emb_dim, proj_dim),\n            nn.GELU(),\n            nn.Dropout(dropout)\n        )\n        # common trunk\n        self.trunk = nn.Sequential(\n            nn.Linear(proj_dim, hid),\n            nn.LayerNorm(hid),\n            nn.GELU(),\n            nn.Dropout(dropout),\n            nn.Linear(hid, proj_dim),\n            nn.GELU(),\n            nn.Dropout(dropout)\n        )\n        # heads\n        self.head_mf = nn.Linear(proj_dim, n_mf) if n_mf>0 else None\n        self.head_bp = nn.Linear(proj_dim, n_bp) if n_bp>0 else None\n        self.head_cc = nn.Linear(proj_dim, n_cc) if n_cc>0 else None\n\n    def forward(self, x):\n        x = self.proj(x)\n        x = self.trunk(x)\n        out_mf = self.head_mf(x) if self.head_mf is not None else None\n        out_bp = self.head_bp(x) if self.head_bp is not None else None\n        out_cc = self.head_cc(x) if self.head_cc is not None else None\n        return out_mf, out_bp, out_cc\n\n# ------------------------\n# Loss helpers (per head)\n# ------------------------\ndef make_loss(outputs, targets):\n    # outputs, targets are tensors (B, C) or None\n    loss = 0.0\n    count = 0\n    criterion = nn.BCEWithLogitsLoss()\n    for out, tgt in zip(outputs, targets):\n        if out is None or tgt.numel()==0:\n            continue\n        loss = loss + criterion(out, tgt)\n        count += 1\n    return loss if count>0 else torch.tensor(0.0, device=DEVICE)\n\n# ------------------------\n# Metrics: simple per-ontology F1 (macro average)\n# ------------------------\ndef threshold_search(y_true: np.ndarray, y_score: np.ndarray):\n    \"\"\"\n    For each class, find best threshold on val set using F1 from precision_recall_curve.\n    Returns vector of thresholds (len = n_classes).\n    \"\"\"\n    n_classes = y_true.shape[1]\n    thrs = np.zeros(n_classes, dtype=float)\n    for j in range(n_classes):\n        yj = y_true[:, j]\n        sj = y_score[:, j]\n        if yj.sum() == 0:\n            thrs[j] = 0.5\n            continue\n        precision, recall, thr = precision_recall_curve(yj, sj)\n        f1 = (2*precision*recall) / (precision+recall+1e-12)\n        idx = np.nanargmax(f1)\n        # thr array len = len(precision)-1; handle boundaries:\n        if idx >= len(thr):\n            chosen = 0.5\n        else:\n            chosen = thr[idx]\n        thrs[j] = float(chosen)\n    return thrs\n\ndef compute_f1_at_thresholds(y_true, y_score, thresholds):\n    y_pred = (y_score >= thresholds).astype(int)\n    # per-class f1 -> mean\n    f1s = []\n    for j in range(y_true.shape[1]):\n        if y_true[:, j].sum()==0:\n            continue\n        f1s.append(f1_score(y_true[:, j], y_pred[:, j], zero_division=0))\n    if len(f1s)==0:\n        return 0.0\n    return float(np.mean(f1s))\n\n# ------------------------\n# Load data\n# ------------------------\nprint(\"Loading embeddings and label matrices...\")\nemb = load_embeddings(EMB_PATH)   # memmap (N, D)\nlab_mf = load_label_npz(LABEL_DIR / \"labels_MF.npz\")\nlab_bp = load_label_npz(LABEL_DIR / \"labels_BP.npz\")\nlab_cc = load_label_npz(LABEL_DIR / \"labels_CC.npz\")\n\nn_proteins, emb_dim = emb.shape\nn_mf = lab_mf.shape[1]\nn_bp = lab_bp.shape[1]\nn_cc = lab_cc.shape[1]\nprint(f\"n_prot={n_proteins}, emb_dim={emb_dim}, MF={n_mf}, BP={n_bp}, CC={n_cc}\")\n\n# ------------------------\n# Train/Val split (stratified by number of labels per protein)\n# ------------------------\nall_idx = np.arange(n_proteins)\n# stratify by label counts (sum MF+BP+CC)\nlabel_counts = (\n    lab_mf.sum(axis=1).A1 +\n    lab_bp.sum(axis=1).A1 +\n    lab_cc.sum(axis=1).A1\n).astype(int)\n\n# create bins\nbins = np.minimum(10, (label_counts // 5) + 1)\ntrain_idx, val_idx = train_test_split(all_idx, test_size=0.15, random_state=SEED, stratify=bins)\n\nprint(\"Train size:\", len(train_idx), \"Val size:\", len(val_idx))\n\n# ------------------------\n# Datasets & Loaders\n# ------------------------\nlabel_mats = {'mf': lab_mf, 'bp': lab_bp, 'cc': lab_cc}\ntrain_ds = EmbLabelDataset(emb, label_mats, train_idx.tolist())\nval_ds   = EmbLabelDataset(emb, label_mats, val_idx.tolist())\n\ntrain_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, collate_fn=collate_fn)\nval_loader   = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, collate_fn=collate_fn)\n\n# ------------------------\n# Model init\n# ------------------------\nmodel = MultiHeadNet(emb_dim=emb_dim, proj_dim=PROJ_DIM, hid=HID, n_mf=n_mf, n_bp=n_bp, n_cc=n_cc, dropout=DROPOUT)\nmodel = model.to(DEVICE)\noptimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\nscaler = torch.cuda.amp.GradScaler(enabled=(DEVICE.type==\"cuda\"))\nscheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)\n\nbest_val = -1.0\npat = 0\n\n# ------------------------\n# Training loop\n# ------------------------\nfor epoch in range(EPOCHS):\n    model.train()\n    epoch_loss = 0.0\n    t0 = time.time()\n    for X, Ym, Yb, Yc in tqdm(train_loader, desc=f\"Train E{epoch}\"):\n        X = X.to(DEVICE)\n        Ym = Ym.to(DEVICE) if Ym.numel() else Ym.to(DEVICE)\n        Yb = Yb.to(DEVICE) if Yb.numel() else Yb.to(DEVICE)\n        Yc = Yc.to(DEVICE) if Yc.numel() else Yc.to(DEVICE)\n\n        optimizer.zero_grad()\n        with torch.cuda.amp.autocast(enabled=(DEVICE.type==\"cuda\")):\n            out_mf, out_bp, out_cc = model(X)\n            loss = make_loss((out_mf, out_bp, out_cc), (Ym, Yb, Yc))\n        scaler.scale(loss).backward()\n        scaler.step(optimizer)\n        scaler.update()\n        epoch_loss += float(loss.detach().cpu().numpy())\n\n    scheduler.step()\n    t1 = time.time()\n\n    # ------------------------\n    # Validation: collect scores\n    # ------------------------\n    model.eval()\n    all_scores = {'mf': [], 'bp': [], 'cc': []}\n    all_trues  = {'mf': [], 'bp': [], 'cc': []}\n\n    with torch.inference_mode():\n        for X, Ym, Yb, Yc in tqdm(val_loader, desc=f\"Val E{epoch}\"):\n            X = X.to(DEVICE)\n            out_mf, out_bp, out_cc = model(X)\n            # move to cpu numpy\n            if out_mf is not None:\n                all_scores['mf'].append(out_mf.sigmoid().cpu().numpy())\n                all_trues ['mf'].append(Ym.cpu().numpy())\n            if out_bp is not None:\n                all_scores['bp'].append(out_bp.sigmoid().cpu().numpy())\n                all_trues ['bp'].append(Yb.cpu().numpy())\n            if out_cc is not None:\n                all_scores['cc'].append(out_cc.sigmoid().cpu().numpy())\n                all_trues ['cc'].append(Yc.cpu().numpy())\n\n    # stack arrays\n    val_scores = {}\n    val_trues = {}\n    for k in all_scores:\n        val_scores[k] = np.vstack(all_scores[k]) if len(all_scores[k])>0 else np.zeros((0,0))\n        val_trues[k]  = np.vstack(all_trues[k])  if len(all_trues[k])>0 else np.zeros((0,0))\n\n    # ------------------------\n    # Threshold tuning & F1 eval (mean-of-heads)\n    # ------------------------\n    val_f1s = {}\n    val_thrs = {}\n    for k in ['mf','bp','cc']:\n        if val_trues[k].size==0:\n            val_f1s[k] = 0.0\n            val_thrs[k] = np.array([])\n            continue\n        # compute per-class thresholds on val (expensive but only once per epoch)\n        th = threshold_search(val_trues[k], val_scores[k])\n        val_thrs[k] = th\n        f1 = compute_f1_at_thresholds(val_trues[k], val_scores[k], th)\n        val_f1s[k] = f1\n\n    mean_val = np.mean([val_f1s['mf'], val_f1s['bp'], val_f1s['cc']])\n    print(f\"E{epoch} loss={epoch_loss:.4f} time={t1-t0:.1f}s MF_f1={val_f1s['mf']:.4f} BP_f1={val_f1s['bp']:.4f} CC_f1={val_f1s['cc']:.4f} mean={mean_val:.4f}\")\n\n    # ------------------------\n    # checkpointing\n    # ------------------------\n    if mean_val > best_val:\n        best_val = mean_val\n        pat = 0\n        ckpt = MODEL_DIR / f\"multihead_epoch{epoch}_mean{mean_val:.4f}.pth\"\n        torch.save({\n            \"model_state\": model.state_dict(),\n            \"optimizer_state\": optimizer.state_dict(),\n            \"epoch\": epoch,\n            \"mean_val\": mean_val,\n            \"thresholds\": {k: val_thrs[k].tolist() for k in val_thrs}\n        }, ckpt)\n        print(\"[OK] Saved checkpoint:\", ckpt)\n    else:\n        pat += 1\n        print(\"[..] no improvement pat=\", pat)\n        if pat >= PATIENCE:\n            print(\"[!!] Early stopping triggered.\")\n            break\n\n# ------------------------\n# Save final thresholds (from best checkpoint if exists)\n\nckpt_files = sorted(MODEL_DIR.glob(\"multihead_epoch*.pth\"))\nif ckpt_files:\n    last_ckpt = ckpt_files[-1]\n    _ser.add_safe_globals([np.dtype, _np.core.multiarray.scalar])\n\n    state = torch.load(last_ckpt, map_location=DEVICE, weights_only=False)\n    thr_out = state.get(\"thresholds\", {})\nelse:\n    thr_out = {k: (val_thrs[k].tolist() if k in val_thrs else []) for k in ['mf','bp','cc']}\n\nwith open(MODEL_DIR / \"thresholds.json\", \"w\") as fh:\n    json.dump({k: (thr_out.get(k, [])) for k in thr_out}, fh)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T09:47:27.650105Z","iopub.execute_input":"2025-11-30T09:47:27.651036Z","iopub.status.idle":"2025-11-30T10:07:22.268099Z","shell.execute_reply.started":"2025-11-30T09:47:27.651001Z","shell.execute_reply":"2025-11-30T10:07:22.267158Z"}},"outputs":[{"name":"stdout","text":"DEVICE: cuda\nLoading embeddings and label matrices...\nLoading embeddings (memmap)... /kaggle/working/embeddings/train_protbert_gpu.npy\nEmbeddings shape: (82404, 1024)\nLoading label npz: /kaggle/working/outputs_labels/labels_MF.npz\nLoading label npz: /kaggle/working/outputs_labels/labels_BP.npz\nLoading label npz: /kaggle/working/outputs_labels/labels_CC.npz\nn_prot=82404, emb_dim=1024, MF=6616, BP=16858, CC=2651\nTrain size: 70043 Val size: 12361\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_47/2313830678.py:215: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  scaler = torch.cuda.amp.GradScaler(enabled=(DEVICE.type==\"cuda\"))\nTrain E0:   0%|          | 0/548 [00:00<?, ?it/s]/tmp/ipykernel_47/2313830678.py:235: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(DEVICE.type==\"cuda\")):\nTrain E0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 548/548 [00:14<00:00, 36.98it/s]\nVal E0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 97/97 [00:03<00:00, 29.22it/s]\n","output_type":"stream"},{"name":"stdout","text":"E0 loss=67.0798 time=14.8s MF_f1=0.0029 BP_f1=0.0030 CC_f1=0.0054 mean=0.0038\n[OK] Saved checkpoint: /kaggle/working/models/multihead_epoch0_mean0.0038.pth\n","output_type":"stream"},{"name":"stderr","text":"Train E1:   0%|          | 0/548 [00:00<?, ?it/s]/tmp/ipykernel_47/2313830678.py:235: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(DEVICE.type==\"cuda\")):\nTrain E1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 548/548 [00:14<00:00, 38.23it/s]\nVal E1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 97/97 [00:03<00:00, 29.26it/s]\n","output_type":"stream"},{"name":"stdout","text":"E1 loss=10.3747 time=14.3s MF_f1=0.0028 BP_f1=0.0029 CC_f1=0.0052 mean=0.0036\n[..] no improvement pat= 1\n","output_type":"stream"},{"name":"stderr","text":"Train E2:   0%|          | 0/548 [00:00<?, ?it/s]/tmp/ipykernel_47/2313830678.py:235: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(DEVICE.type==\"cuda\")):\nTrain E2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 548/548 [00:14<00:00, 38.21it/s]\nVal E2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 97/97 [00:03<00:00, 29.47it/s]\n","output_type":"stream"},{"name":"stdout","text":"E2 loss=10.0806 time=14.3s MF_f1=0.0028 BP_f1=0.0029 CC_f1=0.0051 mean=0.0036\n[..] no improvement pat= 2\n","output_type":"stream"},{"name":"stderr","text":"Train E3:   0%|          | 0/548 [00:00<?, ?it/s]/tmp/ipykernel_47/2313830678.py:235: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(DEVICE.type==\"cuda\")):\nTrain E3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 548/548 [00:14<00:00, 38.66it/s]\nVal E3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 97/97 [00:03<00:00, 29.66it/s]\n","output_type":"stream"},{"name":"stdout","text":"E3 loss=9.9056 time=14.2s MF_f1=0.0027 BP_f1=0.0029 CC_f1=0.0051 mean=0.0036\n[..] no improvement pat= 3\n","output_type":"stream"},{"name":"stderr","text":"Train E4:   0%|          | 0/548 [00:00<?, ?it/s]/tmp/ipykernel_47/2313830678.py:235: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(DEVICE.type==\"cuda\")):\nTrain E4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 548/548 [00:14<00:00, 38.42it/s]\nVal E4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 97/97 [00:03<00:00, 29.53it/s]\n","output_type":"stream"},{"name":"stdout","text":"E4 loss=9.7974 time=14.3s MF_f1=0.0027 BP_f1=0.0029 CC_f1=0.0051 mean=0.0036\n[..] no improvement pat= 4\n","output_type":"stream"},{"name":"stderr","text":"Train E5:   0%|          | 0/548 [00:00<?, ?it/s]/tmp/ipykernel_47/2313830678.py:235: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(DEVICE.type==\"cuda\")):\nTrain E5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 548/548 [00:14<00:00, 38.63it/s]\nVal E5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 97/97 [00:03<00:00, 29.30it/s]\n","output_type":"stream"},{"name":"stdout","text":"E5 loss=9.7342 time=14.2s MF_f1=0.0028 BP_f1=0.0029 CC_f1=0.0052 mean=0.0036\n[..] no improvement pat= 5\n","output_type":"stream"},{"name":"stderr","text":"Train E6:   0%|          | 0/548 [00:00<?, ?it/s]/tmp/ipykernel_47/2313830678.py:235: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(DEVICE.type==\"cuda\")):\nTrain E6: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 548/548 [00:14<00:00, 38.79it/s]\nVal E6: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 97/97 [00:03<00:00, 29.50it/s]\n","output_type":"stream"},{"name":"stdout","text":"E6 loss=9.6840 time=14.1s MF_f1=0.0028 BP_f1=0.0030 CC_f1=0.0051 mean=0.0036\n[..] no improvement pat= 6\n","output_type":"stream"},{"name":"stderr","text":"Train E7:   0%|          | 0/548 [00:00<?, ?it/s]/tmp/ipykernel_47/2313830678.py:235: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(DEVICE.type==\"cuda\")):\nTrain E7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 548/548 [00:14<00:00, 38.67it/s]\nVal E7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 97/97 [00:03<00:00, 29.51it/s]\n","output_type":"stream"},{"name":"stdout","text":"E7 loss=9.6575 time=14.2s MF_f1=0.0028 BP_f1=0.0029 CC_f1=0.0052 mean=0.0037\n[..] no improvement pat= 7\n","output_type":"stream"},{"name":"stderr","text":"Train E8:   0%|          | 0/548 [00:00<?, ?it/s]/tmp/ipykernel_47/2313830678.py:235: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(DEVICE.type==\"cuda\")):\nTrain E8: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 548/548 [00:14<00:00, 38.70it/s]\nVal E8: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 97/97 [00:03<00:00, 29.26it/s]\n","output_type":"stream"},{"name":"stdout","text":"E8 loss=9.6410 time=14.2s MF_f1=0.0030 BP_f1=0.0030 CC_f1=0.0055 mean=0.0038\n[OK] Saved checkpoint: /kaggle/working/models/multihead_epoch8_mean0.0038.pth\n","output_type":"stream"},{"name":"stderr","text":"Train E9:   0%|          | 0/548 [00:00<?, ?it/s]/tmp/ipykernel_47/2313830678.py:235: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(DEVICE.type==\"cuda\")):\nTrain E9: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 548/548 [00:14<00:00, 38.53it/s]\nVal E9: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 97/97 [00:03<00:00, 29.27it/s]\n","output_type":"stream"},{"name":"stdout","text":"E9 loss=9.6300 time=14.2s MF_f1=0.0028 BP_f1=0.0029 CC_f1=0.0052 mean=0.0036\n[..] no improvement pat= 1\n","output_type":"stream"}],"execution_count":32},{"cell_type":"code","source":"# Add safe globals if needed\nimport torch.serialization as _ser\n_ser.add_safe_globals([np.dtype, np.core.multiarray.scalar])\n\n# Load checkpoint safely\nstate = torch.load(last_ckpt, map_location=DEVICE, weights_only=False)\n\n# Restore weights\nmodel.load_state_dict(state[\"model_state\"])\n\n# Restore thresholds (used for converting probabilities to GO predictions)\nthr_out = state[\"thresholds\"]\n\nprint(\"Checkpoint restored from:\", last_ckpt)\nprint(\"Epoch:\", state[\"epoch\"])\nprint(\"Best validation mean:\", state[\"mean_val\"])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T10:23:17.982607Z","iopub.execute_input":"2025-11-30T10:23:17.983117Z","iopub.status.idle":"2025-11-30T10:23:18.121347Z","shell.execute_reply.started":"2025-11-30T10:23:17.983093Z","shell.execute_reply":"2025-11-30T10:23:18.120639Z"}},"outputs":[{"name":"stdout","text":"Checkpoint restored from: /kaggle/working/models/multihead_epoch8_mean0.0038.pth\nEpoch: 8\nBest validation mean: 0.0038362937881450732\n","output_type":"stream"}],"execution_count":37},{"cell_type":"code","source":"model = MultiHeadModel(\n    input_dim=1024,\n    proj_dim=512,\n    mf_dim=len(mf_terms),\n    bp_dim=len(bp_terms),\n    cc_dim=len(cc_terms)\n).to(DEVICE)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T10:29:12.397611Z","iopub.execute_input":"2025-11-30T10:29:12.398303Z","iopub.status.idle":"2025-11-30T10:29:12.497026Z","shell.execute_reply.started":"2025-11-30T10:29:12.398280Z","shell.execute_reply":"2025-11-30T10:29:12.496440Z"}},"outputs":[],"execution_count":45},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\nclass MultiHeadModel(nn.Module):\n    def __init__(self, input_dim=1024, proj_dim=512, hid=1024,\n                 mf_dim=6616, bp_dim=16858, cc_dim=2651,\n                 dropout=0.2):\n        super().__init__()\n\n        # Projection matches proj.0 and proj.1 in checkpoint\n        self.proj = nn.Sequential(\n            nn.LayerNorm(input_dim),            # proj.0\n            nn.Linear(input_dim, proj_dim),     # proj.1\n        )\n\n        # Trunk matches trunk.0, trunk.1, trunk.4 in checkpoint\n        self.trunk = nn.Sequential(\n            nn.Linear(proj_dim, hid),           # trunk.0\n            nn.LayerNorm(hid),                  # trunk.1\n            nn.GELU(),\n            nn.Dropout(dropout),\n            nn.Linear(hid, proj_dim),           # trunk.4\n            nn.GELU(),\n            nn.Dropout(dropout),\n        )\n\n        # Output heads EXACTLY matching checkpoint shapes\n        self.head_mf = nn.Linear(proj_dim, mf_dim)\n        self.head_bp = nn.Linear(proj_dim, bp_dim)\n        self.head_cc = nn.Linear(proj_dim, cc_dim)\n\n    def forward(self, x):\n        x = self.proj(x)\n        x = self.trunk(x)\n        mf = self.head_mf(x)\n        bp = self.head_bp(x)\n        cc = self.head_cc(x)\n        return mf, bp, cc\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T10:35:07.224634Z","iopub.execute_input":"2025-11-30T10:35:07.225372Z","iopub.status.idle":"2025-11-30T10:35:07.232063Z","shell.execute_reply.started":"2025-11-30T10:35:07.225346Z","shell.execute_reply":"2025-11-30T10:35:07.231194Z"}},"outputs":[],"execution_count":51},{"cell_type":"code","source":"BASE = Path(\"/kaggle/working\")\nEMB_TEST = BASE / \"embeddings\" / \"test_protbert_gpu.npy\"\nLABEL_DIR = BASE / \"outputs_labels\"\nMODEL_DIR = BASE / \"models\"\n\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# ===============================\n# LOAD LABEL MAPS & ANCESTORS\n# ===============================\nmaps = json.load(open(LABEL_DIR / \"maps.json\"))\n\nmf_terms = maps[\"mf_terms\"]\nbp_terms = maps[\"bp_terms\"]\ncc_terms = maps[\"cc_terms\"]\n\n# Load ancestors for propagation\ndef load_ancestors():\n    # reuse the build_ancestors function output saved earlier\n    anc_path = LABEL_DIR / \"ancestors.json\"\n    if anc_path.exists():\n        return json.load(open(anc_path))\n    else:\n        raise FileNotFoundError(\"ancestors.json missing! Re-run OBO parser.\")\n\nancestors = load_ancestors()\n\n# ===============================\n# LOAD MODEL\n# ===============================\nckpt_files = sorted(list(MODEL_DIR.glob(\"*.pth\")))\nassert len(ckpt_files) > 0, \"No model checkpoint found!\"\nlast_ckpt = ckpt_files[-1]\nprint(\"Loading:\", last_ckpt)\n\nstate = torch.load(last_ckpt, map_location=DEVICE, weights_only=False)\n\nmodel = MultiHeadModel(\n    input_dim=1024,\n    proj_dim=512,\n    mf_dim=len(mf_terms),\n    bp_dim=len(bp_terms),\n    cc_dim=len(cc_terms)\n).to(DEVICE)\n\nmodel.load_state_dict(state[\"model_state\"])\nthresholds = state[\"thresholds\"]\n\nmodel.eval()\nprint(\"[OK] Model loaded.\")\n\n\n# ===============================\n# LOAD TEST EMBEDDINGS\n# ===============================\nprint(\"Loading test embeddings...\")\ntest_emb = np.load(EMB_TEST, mmap_mode='r')\nprint(\"Test emb shape:\", test_emb.shape)\n\n# ===============================\n# TEST IDS\n# ===============================\nTEST_FASTA = Path(\"/kaggle/input/cafa-6-protein-function-prediction/Test/testsuperset-taxon-list.tsv\")\ntest_ids_df = pd.read_csv(TEST_FASTA, sep=\"\\t\", header=None, names=[\"id\", \"tax\"])\ntest_ids = test_ids_df[\"id\"].tolist()\n\nprint(\"Test IDs:\", len(test_ids))\n\n\n# ===============================\n# INFERENCE\n# ===============================\nbatch_size = 64\npred_rows = []\n\nwith torch.no_grad():\n    for i in tqdm(range(0, len(test_ids), batch_size)):\n        batch_ids = test_ids[i:i+batch_size]\n        idxs = list(range(i, i+len(batch_ids)))\n\n        batch_emb = torch.tensor(test_emb[idxs], dtype=torch.float32).to(DEVICE)\n\n        mf_logits, bp_logits, cc_logits = model(batch_emb)\n        \n        mf_probs = torch.sigmoid(mf_logits).cpu().numpy()\n        bp_probs = torch.sigmoid(bp_logits).cpu().numpy()\n        cc_probs = torch.sigmoid(cc_logits).cpu().numpy()\n\n        for j, pid in enumerate(batch_ids):\n            # Collect predictions\n            preds = []\n\n            # MF\n            for k, p in enumerate(mf_probs[j]):\n                if p >= thresholds[\"mf\"][k]:\n                    preds.append((pid, mf_terms[k], float(p)))\n\n            # BP\n            for k, p in enumerate(bp_probs[j]):\n                if p >= thresholds[\"bp\"][k]:\n                    preds.append((pid, bp_terms[k], float(p)))\n\n            # CC\n            for k, p in enumerate(cc_probs[j]):\n                if p >= thresholds[\"cc\"][k]:\n                    preds.append((pid, cc_terms[k], float(p)))\n\n            # Propagate parents (max rule)\n            propagated = {}\n            for pid2, go, score in preds:\n                for anc in ancestors.get(go, []):\n                    propagated[anc] = max(propagated.get(anc, 0), score)\n\n            # Sort by score desc\n            final_terms = sorted(propagated.items(), key=lambda x: -x[1])\n\n            # Limit to 1500 terms/protein\n            final_terms = final_terms[:1500]\n\n            for go, sc in final_terms:\n                pred_rows.append(f\"{pid}\\t{go}\\t{sc:.3f}\")\n\n\n# ===============================\n# SAVE SUBMISSION\n# ===============================\nSUB_PATH = BASE / \"submission.tsv\"\n\nwith open(SUB_PATH, \"w\") as f:\n    for row in pred_rows:\n        f.write(row + \"\\n\")\n\nprint(\"[OK] Submission saved:\", SUB_PATH)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T10:35:10.149251Z","iopub.execute_input":"2025-11-30T10:35:10.149629Z","iopub.status.idle":"2025-11-30T10:41:13.146872Z","shell.execute_reply.started":"2025-11-30T10:35:10.149605Z","shell.execute_reply":"2025-11-30T10:41:13.145871Z"}},"outputs":[{"name":"stdout","text":"Loading: /kaggle/working/models/multihead_epoch8_mean0.0038.pth\n[OK] Model loaded.\nLoading test embeddings...\nTest emb shape: (224309, 1024)\nTest IDs: 8454\n","output_type":"stream"},{"name":"stderr","text":"100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 133/133 [06:02<00:00,  2.73s/it]","output_type":"stream"},{"name":"stdout","text":"[OK] Submission saved: /kaggle/working/submission.tsv\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":52}]}