{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"tpuV5e8","dataSources":[{"sourceId":116062,"databundleVersionId":14084779,"sourceType":"competition"}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# CAFA-6 Protein Function Prediction: The Ensemble Era \n\nWelcome to this comprehensive pipeline for the **CAFA-6 Competition**!\n\nIn this notebook, we will build a powerful, **multi-model ensemble** system to predict protein functions (GO terms) from amino acid sequences. We will combine the strengths of three state-of-the-art Protein Language Models (pLMs):\n\n1.  **ProtBERT**  (Rostlab)\n2.  **ESM-2**  (Meta AI)\n3.  **ProtT5** (Rostlab)\n\n###  Key Features\n-   **Visual EDA**: Beautiful plots to understand our data.\n-   **GPU Optimization**: Efficient embedding generation.\n-   **Ensemble Learning**: Averaging predictions for robustness.\n-   **Clean Code**: Structured, commented, and easy to follow.\n\nLet's dive in! \n\n**NOTE:** I am a beginner and currently learning stuff so feel free to comment and make corrections","metadata":{}},{"cell_type":"code","source":"!pip install biopython --quiet # I was getting an error so that's why doing this","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T04:38:52.065497Z","iopub.execute_input":"2025-12-01T04:38:52.065890Z","iopub.status.idle":"2025-12-01T04:38:53.811769Z","shell.execute_reply.started":"2025-12-01T04:38:52.065867Z","shell.execute_reply":"2025-12-01T04:38:53.810451Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Imports & Setup\nimport os\nimport sys\nimport gc\nimport json\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import AutoTokenizer, AutoModel, T5EncoderModel\nfrom Bio import SeqIO\nfrom tqdm.auto import tqdm\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import sparse\n\n\n# TPU SUPPORT (ADDED\nUSE_TPU = False\ntry:\n    import torch_xla\n    import torch_xla.core.xla_model as xm\n    import torch_xla.distributed.parallel_loader as pl\n    import torch_xla.debug.metrics as met\n    import torch_xla.utils.utils as xu\n    USE_TPU = True\nexcept ImportError:\n    USE_TPU = False\n\n#  Plotting Style\nsns.set_theme(style=\"whitegrid\", palette=\"viridis\")\nplt.rcParams['figure.figsize'] = (12, 6)\nplt.rcParams['font.size'] = 12\n\n#  Paths\nINPUT_DIR = \"/kaggle/input/cafa-6-protein-function-prediction/\"\nWORKING_DIR = \"/kaggle/working/\"\nOUTPUT_LABELS_DIR = os.path.join(WORKING_DIR, \"outputs_labels\")\nEMBEDDINGS_DIR = os.path.join(WORKING_DIR, \"embeddings\")\nMODELS_DIR = os.path.join(WORKING_DIR, \"models\")\nos.makedirs(OUTPUT_LABELS_DIR, exist_ok=True)\nos.makedirs(EMBEDDINGS_DIR, exist_ok=True)\nos.makedirs(MODELS_DIR, exist_ok=True)\n\nif USE_TPU:\n    device = torch_xla.device()\n    print(\" TPU detected. Using device:\", device)\nelse:\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    print(\" Using device:\", device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T03:46:46.346272Z","iopub.execute_input":"2025-12-01T03:46:46.346550Z","iopub.status.idle":"2025-12-01T03:46:46.354756Z","shell.execute_reply.started":"2025-12-01T03:46:46.346532Z","shell.execute_reply":"2025-12-01T03:46:46.353737Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Part 1: Exploratory Data Analysis (EDA)\nBefore we model, we must understand. Let's load the data and visualize the sequence lengths and GO term distributions.","metadata":{}},{"cell_type":"code","source":"#  Load Data\nprint(\" Loading sequences...\")\ntrain_seqs = {rec.id: str(rec.seq) for rec in SeqIO.parse(os.path.join(INPUT_DIR, \"Train/train_sequences.fasta\"), \"fasta\")}\ntest_seqs = {rec.id: str(rec.seq) for rec in SeqIO.parse(os.path.join(INPUT_DIR, \"Test/testsuperset.fasta\"), \"fasta\")}\ntrain_terms_df = pd.read_csv(os.path.join(INPUT_DIR, \"Train/train_terms.tsv\"), sep=\"\\t\")\n\nprint(f\" Train Sequences: {len(train_seqs):,}\")\nprint(f\" Test Sequences: {len(test_seqs):,}\")\nprint(f\" Train Annotations: {len(train_terms_df):,}\")\n\n# Plot Sequence Length Distribution\ntrain_lens = [len(s) for s in train_seqs.values()]\ntest_lens = [len(s) for s in test_seqs.values()]\n\nplt.figure(figsize=(14, 6))\nsns.histplot(train_lens, color=\"blue\", label=\"Train\", kde=True, alpha=0.5, log_scale=True)\nsns.histplot(test_lens, color=\"orange\", label=\"Test\", kde=True, alpha=0.5, log_scale=True)\nplt.title(\" Sequence Length Distribution (Log Scale)\")\nplt.xlabel(\"Sequence Length\")\nplt.legend()\nplt.show()\n\n#  Plot Top GO Terms\ntop_terms = train_terms_df['term'].value_counts().head(20)\nplt.figure(figsize=(14, 8))\nsns.barplot(x=top_terms.values, y=top_terms.index, palette=\"viridis\")\nplt.title(\" Top 20 Most Frequent GO Terms\")\nplt.xlabel(\"Count\")\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T03:46:46.355191Z","iopub.execute_input":"2025-12-01T03:46:46.355363Z","iopub.status.idle":"2025-12-01T03:46:49.978538Z","shell.execute_reply.started":"2025-12-01T03:46:46.355348Z","shell.execute_reply":"2025-12-01T03:46:49.977354Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"##  Part 2: The Knowledge Graph (Gene Ontology)\nProteins functions are related! If a protein does X, and X is a subclass of Y, it also does Y. We need to parse the **Gene Ontology (GO)** graph to enforce these rules.","metadata":{}},{"cell_type":"code","source":"#  Parse OBO & Build Ancestors\ndef parse_obo(obo_file):\n    terms = {}\n    current_term = None\n    with open(obo_file, 'r') as f:\n        for line in f:\n            line = line.strip()\n            if line == \"[Term]\":\n                if current_term:\n                    terms[current_term['id']] = current_term\n                current_term = {'is_a': [], 'namespace': ''}\n            elif line.startswith(\"id: \"):\n                current_term['id'] = line[4:]\n            elif line.startswith(\"namespace: \"):\n                current_term['namespace'] = line[11:]\n            elif line.startswith(\"is_a: \"):\n                current_term['is_a'].append(line[6:].split(' ! ')[0])\n    if current_term:\n        terms[current_term['id']] = current_term\n    return terms\n\ngo_terms = parse_obo(os.path.join(INPUT_DIR, \"Train/go-basic.obo\"))\nprint(f\" Parsed {len(go_terms):,} GO terms.\")\n\n# Build Ancestors (Transitive Closure)\nancestors = {}\nfor term_id in tqdm(go_terms, desc=\" Building Graph\"):\n    queue = [term_id]\n    visited = set()\n    while queue:\n        curr = queue.pop(0)\n        if curr in visited: continue\n        visited.add(curr)\n        if curr in go_terms:\n            queue.extend(go_terms[curr]['is_a'])\n    ancestors[term_id] = list(visited)\nwith open(os.path.join(OUTPUT_LABELS_DIR, \"ancestors.json\"), \"w\") as f:\n    json.dump(ancestors, f)\nprint(\" Ancestor graph saved.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T03:54:35.664580Z","iopub.execute_input":"2025-12-01T03:54:35.664973Z","iopub.status.idle":"2025-12-01T03:54:36.847803Z","shell.execute_reply.started":"2025-12-01T03:54:35.664941Z","shell.execute_reply":"2025-12-01T03:54:36.846485Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#  Build Label Matrices\nused_terms = set(train_terms_df['term'].unique())\nterms_MF = [t for t in used_terms if t in go_terms and go_terms[t]['namespace'] == 'molecular_function']\nterms_BP = [t for t in used_terms if t in go_terms and go_terms[t]['namespace'] == 'biological_process']\nterms_CC = [t for t in used_terms if t in go_terms and go_terms[t]['namespace'] == 'cellular_component']\n\nterm_maps = {\n    'MF': {t: i for i, t in enumerate(terms_MF)},\n    'BP': {t: i for i, t in enumerate(terms_BP)},\n    'CC': {t: i for i, t in enumerate(terms_CC)}\n}\n\nwith open(os.path.join(OUTPUT_LABELS_DIR, \"maps.json\"), \"w\") as f:\n    json.dump(term_maps, f)\n\ntrain_proteins = list(train_seqs.keys())\nprot_map = {p: i for i, p in enumerate(train_proteins)}\n\nfor ns, terms_list in [('MF', terms_MF), ('BP', terms_BP), ('CC', terms_CC)]:\n    term_map = term_maps[ns]\n    df_ns = train_terms_df[train_terms_df['term'].isin(set(terms_list))]\n    df_ns = df_ns[df_ns['EntryID'].isin(prot_map)]\n    \n    p_indices = df_ns['EntryID'].map(prot_map).values\n    t_indices = df_ns['term'].map(term_map).values\n    \n    mat = sparse.coo_matrix((np.ones(len(p_indices)), (p_indices, t_indices)), \n                            shape=(len(train_proteins), len(terms_list)), dtype=np.int8)\n    sparse.save_npz(os.path.join(OUTPUT_LABELS_DIR, f\"labels_{ns}.npz\"), mat.tocsr())\n    print(f\" Saved {ns} labels: {mat.shape}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T03:54:51.903073Z","iopub.execute_input":"2025-12-01T03:54:51.903311Z","iopub.status.idle":"2025-12-01T03:54:52.269707Z","shell.execute_reply.started":"2025-12-01T03:54:51.903294Z","shell.execute_reply":"2025-12-01T03:54:52.268367Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"##  Part 3: Feature Engineering (Embeddings)\nHere I will now extract rich features from protein sequences using **three** different models. This is the secret sauce! \nWe define a generic embedding function to reuse for all models.","metadata":{}},{"cell_type":"code","source":"def embed_sequences(model_name, seq_dict, batch_size=16, max_len=1024, model_type=\"bert\"):\n    print(f\" Loading {model_name}...\")\n    tokenizer = AutoTokenizer.from_pretrained(model_name, do_lower_case=False)\n    \n    #TPU compatibility: use xm.xla_device()\n    if \"xla\" in str(device):\n        import torch_xla.core.xla_model as xm\n        dev = device\n    else:\n        dev = device\n    \n    if model_type == \"t5\":\n        model = T5EncoderModel.from_pretrained(model_name).to(dev)\n    else:\n        model = AutoModel.from_pretrained(model_name).to(dev)\n    model.eval()\n    \n    seqs = list(seq_dict.values())\n    ids = list(seq_dict.keys())\n    embeddings = []\n    \n    for i in tqdm(range(0, len(seqs), batch_size), desc=f\"Embedding {model_name}\"):\n        batch_seqs = seqs[i:i+batch_size]\n\n        # Space sequences\n        batch_seqs = [\" \".join(list(s)) for s in batch_seqs]\n\n        inputs = tokenizer(batch_seqs, return_tensors=\"pt\", padding=True,\n                           truncation=True, max_length=max_len)\n        inputs = {k: v.to(dev) for k, v in inputs.items()}\n        \n        with torch.no_grad():\n            # TPU autocast wrapper (conditional)\n            if \"xla\" in str(dev):\n                import torch_xla.core.xla_model as xm\n                outputs = model(**inputs)\n                xm.mark_step()   # <<< IMPORTANT: forces TPU execution\n            else:\n                with torch.cuda.amp.autocast():\n                    outputs = model(**inputs)\n\n            # T5 pooling\n            if model_type == \"t5\":\n                emb = outputs.last_hidden_state.mean(dim=1)\n\n            # BERT / ESM pooling\n            else:\n                attention_mask = inputs['attention_mask']\n                last_hidden_state = outputs.last_hidden_state\n                input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n                sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n                sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n                emb = sum_embeddings / sum_mask\n        \n        embeddings.append(emb.cpu().numpy())\n        \n        # TPU step after each batch \n        if \"xla\" in str(dev):\n            import torch_xla.core.xla_model as xm\n            xm.mark_step()\n\n    # Cleanup\n    del model, tokenizer\n    try:\n        torch.cuda.empty_cache()\n    except:\n        pass\n    gc.collect()\n\n    return np.concatenate(embeddings), ids\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T03:54:56.335756Z","iopub.execute_input":"2025-12-01T03:54:56.336135Z","iopub.status.idle":"2025-12-01T03:54:56.343662Z","shell.execute_reply.started":"2025-12-01T03:54:56.336104Z","shell.execute_reply":"2025-12-01T03:54:56.342585Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#  Model 1: ProtBERT\nemb_bert_train, _ = embed_sequences(\"Rostlab/prot_bert\", train_seqs, batch_size=32, model_type=\"bert\")\nnp.save(os.path.join(EMBEDDINGS_DIR, \"train_protbert.npy\"), emb_bert_train)\ndel emb_bert_train\n\nemb_bert_test, test_ids = embed_sequences(\"Rostlab/prot_bert\", test_seqs, batch_size=32, model_type=\"bert\")\nnp.save(os.path.join(EMBEDDINGS_DIR, \"test_protbert.npy\"), emb_bert_test)\nnp.save(os.path.join(EMBEDDINGS_DIR, \"test_ids.npy\"), test_ids)\ndel emb_bert_test\nprint(\" ProtBERT embeddings saved.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#  Model 2: ESM-2 (150M)\nemb_esm_train, _ = embed_sequences(\"facebook/esm2_t30_150M_UR50D\", train_seqs, batch_size=32, model_type=\"esm\")\nnp.save(os.path.join(EMBEDDINGS_DIR, \"train_esm2.npy\"), emb_esm_train)\ndel emb_esm_train\n\nemb_esm_test, _ = embed_sequences(\"facebook/esm2_t30_150M_UR50D\", test_seqs, batch_size=32, model_type=\"esm\")\nnp.save(os.path.join(EMBEDDINGS_DIR, \"test_esm2.npy\"), emb_esm_test)\ndel emb_esm_test\nprint(\" ESM-2 embeddings saved.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#  Model 3: ProtT5 (XL - Half Precision)\n# Note: ProtT5 is large, we might need smaller batch size\nemb_t5_train, _ = embed_sequences(\"Rostlab/prot_t5_xl_half_uniref50-enc\", train_seqs, batch_size=16, model_type=\"t5\")\nnp.save(os.path.join(EMBEDDINGS_DIR, \"train_prott5.npy\"), emb_t5_train)\ndel emb_t5_train\n\nemb_t5_test, _ = embed_sequences(\"Rostlab/prot_t5_xl_half_uniref50-enc\", test_seqs, batch_size=16, model_type=\"t5\")\nnp.save(os.path.join(EMBEDDINGS_DIR, \"test_prott5.npy\"), emb_t5_test)\ndel emb_t5_test\nprint(\" ProtT5 embeddings saved.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"##  Part 4: Training the Multi-Head Classifier\n\nWe will train a separate **Multi-Layer Perceptron (MLP)** for each embedding type. Each MLP has three heads: one for Molecular Function (MF), one for Biological Process (BP), and one for Cellular Component (CC).","metadata":{}},{"cell_type":"code","source":"class MultiHeadMLP(nn.Module):\n    def __init__(self, input_dim, proj_dim, out_dims):\n        super().__init__()\n        self.proj = nn.Sequential(\n            nn.Linear(input_dim, proj_dim),\n            nn.BatchNorm1d(proj_dim),\n            nn.ReLU(),\n            nn.Dropout(0.3)\n        )\n        self.heads = nn.ModuleDict({\n            ns: nn.Linear(proj_dim, dim) for ns, dim in out_dims.items()\n        })\n        \n    def forward(self, x):\n        x = self.proj(x)\n        return {ns: head(x) for ns, head in self.heads.items()}\n\n\nclass ProteinDataset(Dataset):\n    def __init__(self, embeddings, labels_dict):\n        self.embeddings = embeddings\n        self.labels = labels_dict\n    def __len__(self): return len(self.embeddings)\n    def __getitem__(self, idx):\n        emb = torch.tensor(self.embeddings[idx], dtype=torch.float32)\n        lab = {ns: torch.tensor(self.labels[ns][idx], dtype=torch.float32) for ns in self.labels}\n        return emb, lab\n\n\n\n# TPU READY TRAINING (minimal edits only) \ndef train_and_predict(model_name, train_emb_path, test_emb_path, out_dims, labels):\n    print(f\"\\n Training on {model_name} embeddings...\")\n\n    # Load training embeddings\n    train_emb = np.load(train_emb_path)\n    input_dim = train_emb.shape[1]\n    dataset = ProteinDataset(train_emb, labels)\n    dataloader = DataLoader(dataset, batch_size=128, shuffle=True, num_workers=0)\n\n    # TPU ADDITION: detect TPU \n    using_tpu = (\"xla\" in str(device))\n    if using_tpu:\n        import torch_xla.core.xla_model as xm\n\n    # Build model\n    model = MultiHeadMLP(input_dim, 512, out_dims).to(device)\n    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n    criterion = nn.BCEWithLogitsLoss()\n\n    #  AMP only on GPU \n    scaler = torch.cuda.amp.GradScaler(enabled=not using_tpu)\n\n    # TRAINING LOOP\n    for epoch in range(8):\n        model.train()\n        total_loss = 0\n\n        for emb, lab in tqdm(dataloader, desc=f\"Epoch {epoch+1}\", leave=False):\n            emb = emb.to(device)\n            lab = {ns: l.to(device) for ns, l in lab.items()}\n\n            optimizer.zero_grad()\n\n            # TPU: normal forward (no autocast) \n            if using_tpu:\n                outputs = model(emb)\n                loss = sum([criterion(outputs[ns], lab[ns]) for ns in lab])\n                loss.backward()\n                xm.optimizer_step(optimizer)      # <<< TPU-specific\n                xm.mark_step()                    # <<< TPU commit\n            else:\n                # === GPU: autocast + scaler ===\n                with torch.cuda.amp.autocast():\n                    outputs = model(emb)\n                    loss = sum([criterion(outputs[ns], lab[ns]) for ns in lab])\n\n                scaler.scale(loss).backward()\n                scaler.step(optimizer)\n                scaler.update()\n\n            total_loss += loss.item()\n\n        print(f\"  Epoch {epoch+1} Loss: {total_loss/len(dataloader):.4f}\")\n\n    # CLEANUP\n    del dataset, dataloader, train_emb\n    gc.collect()\n\n    # INFERENCE\n    print(f\" Predicting with {model_name}...\")\n    test_emb = np.load(test_emb_path)\n\n    model.eval()\n    probs_all = {ns: [] for ns in out_dims}\n\n    with torch.no_grad():\n        for i in tqdm(range(0, len(test_emb), 256)):\n            batch = torch.tensor(test_emb[i:i+256], dtype=torch.float32).to(device)\n\n            if using_tpu:\n                out = model(batch)\n                xm.mark_step()      # <<< TPU flush\n            else:\n                out = model(batch)\n\n            for ns in out:\n                probs_all[ns].append(torch.sigmoid(out[ns]).cpu().numpy())\n\n    return {ns: np.concatenate(p) for ns, p in probs_all.items()}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T03:46:50.504501Z","iopub.status.idle":"2025-12-01T03:46:50.504693Z","shell.execute_reply.started":"2025-12-01T03:46:50.504587Z","shell.execute_reply":"2025-12-01T03:46:50.504595Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load Labels & Maps\nlabels = {ns: sparse.load_npz(os.path.join(OUTPUT_LABELS_DIR, f\"labels_{ns}.npz\")).toarray() for ns in ['MF', 'BP', 'CC']}\nwith open(os.path.join(OUTPUT_LABELS_DIR, \"maps.json\")) as f: maps = json.load(f)\nout_dims = {\n    \"MF\": len(maps[\"mf_terms\"]),\n    \"BP\": len(maps[\"bp_terms\"]),\n    \"CC\": len(maps[\"cc_terms\"]),\n}\n\n\n#  Train & Predict All Models\npreds_bert = train_and_predict(\"ProtBERT\", \n                               os.path.join(EMBEDDINGS_DIR, \"train_protbert.npy\"), \n                               os.path.join(EMBEDDINGS_DIR, \"test_protbert.npy\"), out_dims, labels)\n\npreds_esm = train_and_predict(\"ESM2\", \n                              os.path.join(EMBEDDINGS_DIR, \"train_esm2.npy\"), \n                              os.path.join(EMBEDDINGS_DIR, \"test_esm2.npy\"), out_dims, labels)\n\npreds_t5 = train_and_predict(\"ProtT5\", \n                             os.path.join(EMBEDDINGS_DIR, \"train_prott5.npy\"), \n                             os.path.join(EMBEDDINGS_DIR, \"test_prott5.npy\"), out_dims, labels)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T03:46:50.505247Z","iopub.status.idle":"2025-12-01T03:46:50.505551Z","shell.execute_reply.started":"2025-12-01T03:46:50.505351Z","shell.execute_reply":"2025-12-01T03:46:50.505359Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"##  Part 5: Ensemble & Submission\nWe average the probabilities from all three models. Then, we propagate the scores up the GO hierarchy using the **Max Rule** (if a child is predicted with score X, the parent must have score at least X).","metadata":{}},{"cell_type":"code","source":"#  Ensemble Averaging\nprint(\" Ensembling predictions...\")\nfinal_preds = {}\nfor ns in ['MF', 'BP', 'CC']:\n    final_preds[ns] = (preds_bert[ns] + preds_esm[ns] + preds_t5[ns]) / 3.0\n\n#  Propagate & Format\nwith open(os.path.join(OUTPUT_LABELS_DIR, \"ancestors.json\")) as f: ancestors = json.load(f)\nrev_maps = {ns: {v: k for k, v in m.items()} for ns, m in maps.items()}\ntest_ids = np.load(os.path.join(EMBEDDINGS_DIR, \"test_ids.npy\"))\n\nsubmission_lines = []\nprint(\" Generating submission file...\")\n\nfor i, pid in tqdm(enumerate(test_ids), total=len(test_ids)):\n    prot_lines = []\n    for ns in ['MF', 'BP', 'CC']:\n        scores = final_preds[ns][i]\n        # Optimization: Only consider terms with score > 0.001 to speed up propagation\n        term_scores = {rev_maps[ns][idx]: float(s) for idx, s in enumerate(scores) if s > 0.001}\n        \n        # Propagation\n        prop_scores = term_scores.copy()\n        for t_id, score in term_scores.items():\n            if t_id in ancestors:\n                for anc in ancestors[t_id]:\n                    prop_scores[anc] = max(prop_scores.get(anc, 0), score)\n        \n        for t_id, score in prop_scores.items():\n            prot_lines.append((t_id, score))\n            \n    # Top 1500 per protein\n    prot_lines.sort(key=lambda x: x[1], reverse=True)\n    for t_id, score in prot_lines[:1500]:\n        submission_lines.append(f\"{pid}\\t{t_id}\\t{score:.3f}\")\n\nwith open(os.path.join(WORKING_DIR, \"submission.tsv\"), \"w\") as f:\n    f.write(\"\\n\".join(submission_lines))\n\nprint(\" DONE! Submission saved to submission.tsv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T03:46:50.505922Z","iopub.status.idle":"2025-12-01T03:46:50.506341Z","shell.execute_reply.started":"2025-12-01T03:46:50.506006Z","shell.execute_reply":"2025-12-01T03:46:50.506014Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#  Ensemble Averaging\nprint(\" Ensembling predictions...\")\nfinal_preds = {\n    \"MF\": (preds_bert[\"MF\"] + preds_esm[\"MF\"] + preds_t5[\"MF\"]) / 3.0,\n    \"BP\": (preds_bert[\"BP\"] + preds_esm[\"BP\"] + preds_t5[\"BP\"]) / 3.0,\n    \"CC\": (preds_bert[\"CC\"] + preds_esm[\"CC\"] + preds_t5[\"CC\"]) / 3.0,\n}\n\n# Load ancestors\nwith open(os.path.join(OUTPUT_LABELS_DIR, \"ancestors.json\")) as f:\n    ancestors = json.load(f)\n\n# Correct reverse maps\nrev_maps = {\n    \"MF\": {i: go for i, go in enumerate(maps[\"mf_terms\"])},\n    \"BP\": {i: go for i, go in enumerate(maps[\"bp_terms\"])},\n    \"CC\": {i: go for i, go in enumerate(maps[\"cc_terms\"])},\n}\n\n# Test IDs\ntest_ids = np.load(os.path.join(EMBEDDINGS_DIR, \"test_ids.npy\"))\n\nsubmission_lines = []\nprint(\" Generating submission file...\")\n\nfor i, pid in tqdm(enumerate(test_ids), total=len(test_ids)):\n    prot_lines = []\n\n    for ns in [\"MF\", \"BP\", \"CC\"]:\n        scores = final_preds[ns][i]\n\n        # Only keep meaningful\n        term_scores = {\n            rev_maps[ns][idx]: float(s)\n            for idx, s in enumerate(scores)\n            if s > 0.001\n        }\n\n        # Propagation\n        prop_scores = term_scores.copy()\n\n        for go_id, score in term_scores.items():\n            if go_id in ancestors:\n                for anc in ancestors[go_id]:\n                    prop_scores[anc] = max(prop_scores.get(anc, 0), score)\n\n        # Add to list\n        for go_id, score in prop_scores.items():\n            prot_lines.append((go_id, score))\n\n    # Top 1500\n    prot_lines.sort(key=lambda x: x[1], reverse=True)\n    for go_id, score in prot_lines[:1500]:\n        submission_lines.append(f\"{pid}\\t{go_id}\\t{score:.3f}\")\n\n# Save file\nout_path = os.path.join(WORKING_DIR, \"submission.tsv\")\nwith open(out_path, \"w\") as f:\n    f.write(\"\\n\".join(submission_lines))\n\nprint(\" DONE! Submission saved to submission.tsv\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T03:46:50.506681Z","iopub.status.idle":"2025-12-01T03:46:50.506876Z","shell.execute_reply.started":"2025-12-01T03:46:50.506765Z","shell.execute_reply":"2025-12-01T03:46:50.506773Z"}},"outputs":[],"execution_count":null}]}