{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"tpuV5e8","dataSources":[{"sourceId":116062,"databundleVersionId":14084779,"sourceType":"competition"}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# CAFA-6 Protein Function Prediction: The Ensemble Era \n\nWelcome to this comprehensive pipeline for the **CAFA-6 Competition**!\n\nIn this notebook, we will build a powerful, **multi-model ensemble** system to predict protein functions (GO terms) from amino acid sequences. We will combine the strengths of three state-of-the-art Protein Language Models (pLMs):\n\n1.  **ProtBERT**  (Rostlab)\n2.  **ESM-2**  (Meta AI)\n3.  **ProtT5** (Rostlab)\n\n###  Key Features\n-   **Visual EDA**: Beautiful plots to understand our data.\n-   **GPU Optimization**: Efficient embedding generation.\n-   **Ensemble Learning**: Averaging predictions for robustness.\n-   **Clean Code**: Structured, commented, and easy to follow.\n\nLet's dive in! \n\n**NOTE:** I am a beginner and currently learning stuff so feel free to comment and make corrections","metadata":{}},{"cell_type":"code","source":"# Imports & Setup\nimport os\nimport sys\nimport gc\nimport json\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import AutoTokenizer, AutoModel, T5EncoderModel\nfrom Bio import SeqIO\nfrom tqdm.auto import tqdm\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import sparse\n\n#  Plotting Style\nsns.set_theme(style=\"whitegrid\", palette=\"viridis\")\nplt.rcParams['figure.figsize'] = (12, 6)\nplt.rcParams['font.size'] = 12\n\n#  Paths\nINPUT_DIR = \"/kaggle/input/cafa-6-protein-function-prediction/\"\nWORKING_DIR = \"/kaggle/working/\"\nOUTPUT_LABELS_DIR = os.path.join(WORKING_DIR, \"outputs_labels\")\nEMBEDDINGS_DIR = os.path.join(WORKING_DIR, \"embeddings\")\nMODELS_DIR = os.path.join(WORKING_DIR, \"models\")\n\nfor d in [OUTPUT_LABELS_DIR, EMBEDDINGS_DIR, MODELS_DIR]:\n    os.makedirs(d, exist_ok=True)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\" Setup complete. Using device: {device}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Part 1: Exploratory Data Analysis (EDA)\nBefore we model, we must understand. Let's load the data and visualize the sequence lengths and GO term distributions.","metadata":{}},{"cell_type":"code","source":"#  Load Data\nprint(\" Loading sequences...\")\ntrain_seqs = {rec.id: str(rec.seq) for rec in SeqIO.parse(os.path.join(INPUT_DIR, \"Train/train_sequences.fasta\"), \"fasta\")}\ntest_seqs = {rec.id: str(rec.seq) for rec in SeqIO.parse(os.path.join(INPUT_DIR, \"Test/testsuperset.fasta\"), \"fasta\")}\ntrain_terms_df = pd.read_csv(os.path.join(INPUT_DIR, \"Train/train_terms.tsv\"), sep=\"\\t\")\n\nprint(f\"ðŸ”¹ Train Sequences: {len(train_seqs):,}\")\nprint(f\"ðŸ”¹ Test Sequences: {len(test_seqs):,}\")\nprint(f\"ðŸ”¹ Train Annotations: {len(train_terms_df):,}\")\n\n# Plot Sequence Length Distribution\ntrain_lens = [len(s) for s in train_seqs.values()]\ntest_lens = [len(s) for s in test_seqs.values()]\n\nplt.figure(figsize=(14, 6))\nsns.histplot(train_lens, color=\"blue\", label=\"Train\", kde=True, alpha=0.5, log_scale=True)\nsns.histplot(test_lens, color=\"orange\", label=\"Test\", kde=True, alpha=0.5, log_scale=True)\nplt.title(\" Sequence Length Distribution (Log Scale)\")\nplt.xlabel(\"Sequence Length\")\nplt.legend()\nplt.show()\n\n#  Plot Top GO Terms\ntop_terms = train_terms_df['term'].value_counts().head(20)\nplt.figure(figsize=(14, 8))\nsns.barplot(x=top_terms.values, y=top_terms.index, palette=\"viridis\")\nplt.title(\" Top 20 Most Frequent GO Terms\")\nplt.xlabel(\"Count\")\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"##  Part 2: The Knowledge Graph (Gene Ontology)\nProteins functions are related! If a protein does X, and X is a subclass of Y, it also does Y. We need to parse the **Gene Ontology (GO)** graph to enforce these rules.","metadata":{}},{"cell_type":"code","source":"#  Parse OBO & Build Ancestors\ndef parse_obo(obo_file):\n    terms = {}\n    current_term = None\n    with open(obo_file, 'r') as f:\n        for line in f:\n            line = line.strip()\n            if line == \"[Term]\":\n                if current_term:\n                    terms[current_term['id']] = current_term\n                current_term = {'is_a': [], 'namespace': ''}\n            elif line.startswith(\"id: \"):\n                current_term['id'] = line[4:]\n            elif line.startswith(\"namespace: \"):\n                current_term['namespace'] = line[11:]\n            elif line.startswith(\"is_a: \"):\n                current_term['is_a'].append(line[6:].split(' ! ')[0])\n    if current_term:\n        terms[current_term['id']] = current_term\n    return terms\n\ngo_terms = parse_obo(os.path.join(INPUT_DIR, \"Train/go-basic.obo\"))\nprint(f\" Parsed {len(go_terms):,} GO terms.\")\n\n# Build Ancestors (Transitive Closure)\nancestors = {}\nfor term_id in tqdm(go_terms, desc=\"ðŸ•¸ï¸ Building Graph\"):\n    queue = [term_id]\n    visited = set()\n    while queue:\n        curr = queue.pop(0)\n        if curr in visited: continue\n        visited.add(curr)\n        if curr in go_terms:\n            queue.extend(go_terms[curr]['is_a'])\n    ancestors[term_id] = list(visited)\n\nwith open(os.path.join(OUTPUT_LABELS_DIR, \"ancestors.json\"), \"w\") as f:\n    json.dump(ancestors, f)\nprint(\" Ancestor graph saved.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#  Build Label Matrices\nused_terms = set(train_terms_df['term'].unique())\nterms_MF = [t for t in used_terms if t in go_terms and go_terms[t]['namespace'] == 'molecular_function']\nterms_BP = [t for t in used_terms if t in go_terms and go_terms[t]['namespace'] == 'biological_process']\nterms_CC = [t for t in used_terms if t in go_terms and go_terms[t]['namespace'] == 'cellular_component']\n\nterm_maps = {\n    'MF': {t: i for i, t in enumerate(terms_MF)},\n    'BP': {t: i for i, t in enumerate(terms_BP)},\n    'CC': {t: i for i, t in enumerate(terms_CC)}\n}\n\nwith open(os.path.join(OUTPUT_LABELS_DIR, \"maps.json\"), \"w\") as f:\n    json.dump(term_maps, f)\n\ntrain_proteins = list(train_seqs.keys())\nprot_map = {p: i for i, p in enumerate(train_proteins)}\n\nfor ns, terms_list in [('MF', terms_MF), ('BP', terms_BP), ('CC', terms_CC)]:\n    term_map = term_maps[ns]\n    df_ns = train_terms_df[train_terms_df['term'].isin(set(terms_list))]\n    df_ns = df_ns[df_ns['EntryID'].isin(prot_map)]\n    \n    p_indices = df_ns['EntryID'].map(prot_map).values\n    t_indices = df_ns['term'].map(term_map).values\n    \n    mat = sparse.coo_matrix((np.ones(len(p_indices)), (p_indices, t_indices)), \n                            shape=(len(train_proteins), len(terms_list)), dtype=np.int8)\n    sparse.save_npz(os.path.join(OUTPUT_LABELS_DIR, f\"labels_{ns}.npz\"), mat.tocsr())\n    print(f\" Saved {ns} labels: {mat.shape}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"##  Part 3: Feature Engineering (Embeddings)\nHere I will now extract rich features from protein sequences using **three** different models. This is the secret sauce! \nWe define a generic embedding function to reuse for all models.","metadata":{}},{"cell_type":"code","source":"def embed_sequences(model_name, seq_dict, batch_size=16, max_len=1024, model_type=\"bert\"):\n    print(f\" Loading {model_name}...\")\n    tokenizer = AutoTokenizer.from_pretrained(model_name, do_lower_case=False)\n    \n    if model_type == \"t5\":\n        model = T5EncoderModel.from_pretrained(model_name).to(device)\n    else:\n        model = AutoModel.from_pretrained(model_name).to(device)\n    model.eval()\n    \n    seqs = list(seq_dict.values())\n    ids = list(seq_dict.keys())\n    embeddings = []\n    \n    for i in tqdm(range(0, len(seqs), batch_size), desc=f\"Embedding {model_name}\"):\n        batch_seqs = seqs[i:i+batch_size]\n        # Space sequences for BERT/ESM if needed, T5 usually doesn't need spaces but ProtT5 might\n        if \"t5\" not in model_name:\n            batch_seqs = [\" \".join(list(s)) for s in batch_seqs]\n        else:\n            batch_seqs = [\" \".join(list(s)) for s in batch_seqs] # ProtT5 also expects spaced inputs\n\n        inputs = tokenizer(batch_seqs, return_tensors=\"pt\", padding=True, truncation=True, max_length=max_len)\n        inputs = {k: v.to(device) for k, v in inputs.items()}\n        \n        with torch.no_grad():\n            with torch.cuda.amp.autocast():\n                outputs = model(**inputs)\n                if model_type == \"t5\":\n                    # T5 Encoder: last_hidden_state\n                    emb = outputs.last_hidden_state.mean(dim=1)\n                else:\n                    # BERT/ESM: Mean pooling with attention mask\n                    attention_mask = inputs['attention_mask']\n                    last_hidden_state = outputs.last_hidden_state\n                    input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n                    sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n                    sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n                    emb = sum_embeddings / sum_mask\n        \n        embeddings.append(emb.cpu().numpy())\n    \n    del model, tokenizer\n    torch.cuda.empty_cache()\n    gc.collect()\n    return np.concatenate(embeddings), ids","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#  Model 1: ProtBERT\nemb_bert_train, _ = embed_sequences(\"Rostlab/prot_bert\", train_seqs, batch_size=32, model_type=\"bert\")\nnp.save(os.path.join(EMBEDDINGS_DIR, \"train_protbert.npy\"), emb_bert_train)\ndel emb_bert_train\n\nemb_bert_test, test_ids = embed_sequences(\"Rostlab/prot_bert\", test_seqs, batch_size=32, model_type=\"bert\")\nnp.save(os.path.join(EMBEDDINGS_DIR, \"test_protbert.npy\"), emb_bert_test)\nnp.save(os.path.join(EMBEDDINGS_DIR, \"test_ids.npy\"), test_ids)\ndel emb_bert_test\nprint(\" ProtBERT embeddings saved.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#  Model 2: ESM-2 (150M)\nemb_esm_train, _ = embed_sequences(\"facebook/esm2_t30_150M_UR50D\", train_seqs, batch_size=32, model_type=\"esm\")\nnp.save(os.path.join(EMBEDDINGS_DIR, \"train_esm2.npy\"), emb_esm_train)\ndel emb_esm_train\n\nemb_esm_test, _ = embed_sequences(\"facebook/esm2_t30_150M_UR50D\", test_seqs, batch_size=32, model_type=\"esm\")\nnp.save(os.path.join(EMBEDDINGS_DIR, \"test_esm2.npy\"), emb_esm_test)\ndel emb_esm_test\nprint(\" ESM-2 embeddings saved.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#  Model 3: ProtT5 (XL - Half Precision)\n# Note: ProtT5 is large, we might need smaller batch size\nemb_t5_train, _ = embed_sequences(\"Rostlab/prot_t5_xl_half_uniref50-enc\", train_seqs, batch_size=16, model_type=\"t5\")\nnp.save(os.path.join(EMBEDDINGS_DIR, \"train_prott5.npy\"), emb_t5_train)\ndel emb_t5_train\n\nemb_t5_test, _ = embed_sequences(\"Rostlab/prot_t5_xl_half_uniref50-enc\", test_seqs, batch_size=16, model_type=\"t5\")\nnp.save(os.path.join(EMBEDDINGS_DIR, \"test_prott5.npy\"), emb_t5_test)\ndel emb_t5_test\nprint(\" ProtT5 embeddings saved.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"##  Part 4: Training the Multi-Head Classifier\n\nWe will train a separate **Multi-Layer Perceptron (MLP)** for each embedding type. Each MLP has three heads: one for Molecular Function (MF), one for Biological Process (BP), and one for Cellular Component (CC).","metadata":{}},{"cell_type":"code","source":"class MultiHeadMLP(nn.Module):\n    def __init__(self, input_dim, proj_dim, out_dims):\n        super().__init__()\n        self.proj = nn.Sequential(\n            nn.Linear(input_dim, proj_dim),\n            nn.BatchNorm1d(proj_dim),\n            nn.ReLU(),\n            nn.Dropout(0.3)\n        )\n        self.heads = nn.ModuleDict({\n            ns: nn.Linear(proj_dim, dim) for ns, dim in out_dims.items()\n        })\n        \n    def forward(self, x):\n        x = self.proj(x)\n        return {ns: head(x) for ns, head in self.heads.items()}\n\nclass ProteinDataset(Dataset):\n    def __init__(self, embeddings, labels_dict):\n        self.embeddings = embeddings\n        self.labels = labels_dict\n    def __len__(self): return len(self.embeddings)\n    def __getitem__(self, idx):\n        emb = torch.tensor(self.embeddings[idx], dtype=torch.float32)\n        lab = {ns: torch.tensor(self.labels[ns][idx], dtype=torch.float32) for ns in self.labels}\n        return emb, lab\n\ndef train_and_predict(model_name, train_emb_path, test_emb_path, out_dims, labels):\n    print(f\"\\n Training on {model_name} embeddings...\")\n    train_emb = np.load(train_emb_path)\n    input_dim = train_emb.shape[1]\n    dataset = ProteinDataset(train_emb, labels)\n    dataloader = DataLoader(dataset, batch_size=128, shuffle=True, num_workers=0)\n    \n    model = MultiHeadMLP(input_dim, 512, out_dims).to(device)\n    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n    criterion = nn.BCEWithLogitsLoss()\n    scaler = torch.cuda.amp.GradScaler()\n    \n    for epoch in range(8): # 8 epochs per model\n        model.train()\n        total_loss = 0\n        for emb, lab in tqdm(dataloader, desc=f\"Epoch {epoch+1}\", leave=False):\n            emb = emb.to(device)\n            lab = {ns: l.to(device) for ns, l in lab.items()}\n            optimizer.zero_grad()\n            with torch.cuda.amp.autocast():\n                outputs = model(emb)\n                loss = sum([criterion(outputs[ns], lab[ns]) for ns in lab])\n            scaler.scale(loss).backward()\n            scaler.step(optimizer)\n            scaler.update()\n            total_loss += loss.item()\n        print(f\"  Epoch {epoch+1} Loss: {total_loss/len(dataloader):.4f}\")\n    \n    # Inference\n    print(f\" Predicting with {model_name}...\")\n    del dataset, dataloader, train_emb\n    gc.collect()\n    \n    test_emb = np.load(test_emb_path)\n    model.eval()\n    probs_all = {ns: [] for ns in out_dims}\n    \n    with torch.no_grad():\n        for i in tqdm(range(0, len(test_emb), 256)):\n            batch = torch.tensor(test_emb[i:i+256], dtype=torch.float32).to(device)\n            out = model(batch)\n            for ns in out:\n                probs_all[ns].append(torch.sigmoid(out[ns]).cpu().numpy())\n                \n    return {ns: np.concatenate(p) for ns, p in probs_all.items()}","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load Labels & Maps\nlabels = {ns: sparse.load_npz(os.path.join(OUTPUT_LABELS_DIR, f\"labels_{ns}.npz\")).toarray() for ns in ['MF', 'BP', 'CC']}\nwith open(os.path.join(OUTPUT_LABELS_DIR, \"maps.json\")) as f: maps = json.load(f)\nout_dims = {ns: len(maps[ns]) for ns in maps}\n\n#  Train & Predict All Models\npreds_bert = train_and_predict(\"ProtBERT\", \n                               os.path.join(EMBEDDINGS_DIR, \"train_protbert.npy\"), \n                               os.path.join(EMBEDDINGS_DIR, \"test_protbert.npy\"), out_dims, labels)\n\npreds_esm = train_and_predict(\"ESM2\", \n                              os.path.join(EMBEDDINGS_DIR, \"train_esm2.npy\"), \n                              os.path.join(EMBEDDINGS_DIR, \"test_esm2.npy\"), out_dims, labels)\n\npreds_t5 = train_and_predict(\"ProtT5\", \n                             os.path.join(EMBEDDINGS_DIR, \"train_prott5.npy\"), \n                             os.path.join(EMBEDDINGS_DIR, \"test_prott5.npy\"), out_dims, labels)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"##  Part 5: Ensemble & Submission\nWe average the probabilities from all three models. Then, we propagate the scores up the GO hierarchy using the **Max Rule** (if a child is predicted with score X, the parent must have score at least X).","metadata":{}},{"cell_type":"code","source":"#  Ensemble Averaging\nprint(\" Ensembling predictions...\")\nfinal_preds = {}\nfor ns in ['MF', 'BP', 'CC']:\n    final_preds[ns] = (preds_bert[ns] + preds_esm[ns] + preds_t5[ns]) / 3.0\n\n#  Propagate & Format\nwith open(os.path.join(OUTPUT_LABELS_DIR, \"ancestors.json\")) as f: ancestors = json.load(f)\nrev_maps = {ns: {v: k for k, v in m.items()} for ns, m in maps.items()}\ntest_ids = np.load(os.path.join(EMBEDDINGS_DIR, \"test_ids.npy\"))\n\nsubmission_lines = []\nprint(\" Generating submission file...\")\n\nfor i, pid in tqdm(enumerate(test_ids), total=len(test_ids)):\n    prot_lines = []\n    for ns in ['MF', 'BP', 'CC']:\n        scores = final_preds[ns][i]\n        # Optimization: Only consider terms with score > 0.001 to speed up propagation\n        term_scores = {rev_maps[ns][idx]: float(s) for idx, s in enumerate(scores) if s > 0.001}\n        \n        # Propagation\n        prop_scores = term_scores.copy()\n        for t_id, score in term_scores.items():\n            if t_id in ancestors:\n                for anc in ancestors[t_id]:\n                    prop_scores[anc] = max(prop_scores.get(anc, 0), score)\n        \n        for t_id, score in prop_scores.items():\n            prot_lines.append((t_id, score))\n            \n    # Top 1500 per protein\n    prot_lines.sort(key=lambda x: x[1], reverse=True)\n    for t_id, score in prot_lines[:1500]:\n        submission_lines.append(f\"{pid}\\t{t_id}\\t{score:.3f}\")\n\nwith open(os.path.join(WORKING_DIR, \"submission.tsv\"), \"w\") as f:\n    f.write(\"\\n\".join(submission_lines))\n\nprint(\" DONE! Submission saved to submission.tsv\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}