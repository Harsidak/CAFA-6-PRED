{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":116062,"databundleVersionId":14875579,"sourceType":"competition"},{"sourceId":14154175,"sourceType":"datasetVersion","datasetId":8932669}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# CAFA-6 Protein Function Prediction: The Ensemble Era \n\nWelcome to this comprehensive pipeline for the **CAFA-6 Competition**!\n\nIn this notebook, we will build a powerful, **multi-model ensemble** system to predict protein functions (GO terms) from amino acid sequences. We will combine the strengths of three state-of-the-art Protein Language Models (pLMs):\n\n1.  **ProtBERT**  (Rostlab)\n2.  **ESM-2**  (Meta AI)\n\n###  Key Features\n-   **Visual EDA**: Beautiful plots to understand our data.\n-   **GPU Optimization**: Efficient embedding generation.\n-   **Ensemble Learning**: Averaging predictions for robustness.\n-   **Clean Code**: Structured, commented, and easy to follow.\n\nLet's dive in! ","metadata":{}},{"cell_type":"code","source":"!pip install biopython --quiet # I was getting an error so that's why doing this","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Imports & Setup\nimport os\nimport sys\nimport gc\nimport json\nimport re\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import AutoTokenizer, AutoModel, T5EncoderModel\nfrom Bio import SeqIO\nfrom tqdm.auto import tqdm\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import sparse\nfrom torch.utils.data import TensorDataset, DataLoader\nfrom multiprocessing import Pool, cpu_count\nimport glob\n# TPU SUPPORT (ADDED\nUSE_TPU = False\ntry:\n    import torch_xla\n    import torch_xla.core.xla_model as xm\n    import torch_xla.distributed.parallel_loader as pl\n    import torch_xla.debug.metrics as met\n    import torch_xla.utils.utils as xu\n    USE_TPU = True\nexcept ImportError:\n    USE_TPU = False\n\n#  Plotting Style\nsns.set_theme(style=\"whitegrid\", palette=\"viridis\")\nplt.rcParams['figure.figsize'] = (12, 6)\nplt.rcParams['font.size'] = 12\n\n#  Paths\nINPUT_DIR = \"/kaggle/input/cafa-6-protein-function-prediction/\"\nWORKING_DIR = \"/kaggle/working/\"\nOUTPUT_LABELS_DIR = os.path.join(WORKING_DIR, \"outputs_labels\")\nEMBEDDINGS_DIR = os.path.join(WORKING_DIR, \"embeddings\")\nMODELS_DIR = os.path.join(WORKING_DIR, \"models\")\nos.makedirs(OUTPUT_LABELS_DIR, exist_ok=True)\nos.makedirs(EMBEDDINGS_DIR, exist_ok=True)\nos.makedirs(MODELS_DIR, exist_ok=True)\n\nif USE_TPU:\n    device = torch_xla.device()\n    print(\" TPU detected. Using device:\", device)\nelse:\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    print(\" Using device:\", device)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Part 1: Exploratory Data Analysis (EDA)\nBefore we model, we must understand. Let's load the data and visualize the sequence lengths and GO term distributions.","metadata":{}},{"cell_type":"code","source":"#  Load Data\nprint(\" Loading sequences...\")\ntrain_seqs = {rec.id: str(rec.seq) for rec in SeqIO.parse(os.path.join(INPUT_DIR, \"Train/train_sequences.fasta\"), \"fasta\")}\ntest_seqs = {rec.id: str(rec.seq) for rec in SeqIO.parse(os.path.join(INPUT_DIR, \"Test/testsuperset.fasta\"), \"fasta\")}\ntrain_terms_df = pd.read_csv(os.path.join(INPUT_DIR, \"Train/train_terms.tsv\"), sep=\"\\t\")\n\nprint(f\" Train Sequences: {len(train_seqs):,}\")\nprint(f\" Test Sequences: {len(test_seqs):,}\")\nprint(f\" Train Annotations: {len(train_terms_df):,}\")\n\n# Plot Sequence Length Distribution\ntrain_lens = [len(s) for s in train_seqs.values()]\ntest_lens = [len(s) for s in test_seqs.values()]\n\nplt.figure(figsize=(14, 6))\nsns.histplot(train_lens, color=\"blue\", label=\"Train\", kde=True, alpha=0.5, log_scale=True)\nsns.histplot(test_lens, color=\"orange\", label=\"Test\", kde=True, alpha=0.5, log_scale=True)\nplt.title(\" Sequence Length Distribution (Log Scale)\")\nplt.xlabel(\"Sequence Length\")\nplt.legend()\nplt.show()\n\n#  Plot Top GO Terms\ntop_terms = train_terms_df['term'].value_counts().head(20)\nplt.figure(figsize=(14, 8))\nsns.barplot(x=top_terms.values, y=top_terms.index, palette=\"viridis\")\nplt.title(\" Top 20 Most Frequent GO Terms\")\nplt.xlabel(\"Count\")\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"##  Part 2: The Knowledge Graph (Gene Ontology)\nProteins functions are related! If a protein does X, and X is a subclass of Y, it also does Y. We need to parse the **Gene Ontology (GO)** graph to enforce these rules.","metadata":{}},{"cell_type":"code","source":"#  Parse OBO & Build Ancestors\ndef parse_obo(obo_file):\n    terms = {}\n    current_term = None\n    with open(obo_file, 'r') as f:\n        for line in f:\n            line = line.strip()\n            if line == \"[Term]\":\n                if current_term:\n                    terms[current_term['id']] = current_term\n                current_term = {'is_a': [], 'namespace': ''}\n            elif line.startswith(\"id: \"):\n                current_term['id'] = line[4:]\n            elif line.startswith(\"namespace: \"):\n                current_term['namespace'] = line[11:]\n            elif line.startswith(\"is_a: \"):\n                current_term['is_a'].append(line[6:].split(' ! ')[0])\n    if current_term:\n        terms[current_term['id']] = current_term\n    return terms\n\ngo_terms = parse_obo(os.path.join(INPUT_DIR, \"Train/go-basic.obo\"))\nprint(f\" Parsed {len(go_terms):,} GO terms.\")\n\n# Build Ancestors (Transitive Closure)\nancestors = {}\nfor term_id in tqdm(go_terms, desc=\" Building Graph\"):\n    queue = [term_id]\n    visited = set()\n    while queue:\n        curr = queue.pop(0)\n        if curr in visited: continue\n        visited.add(curr)\n        if curr in go_terms:\n            queue.extend(go_terms[curr]['is_a'])\n    ancestors[term_id] = list(visited)\nwith open(os.path.join(OUTPUT_LABELS_DIR, \"ancestors.json\"), \"w\") as f:\n    json.dump(ancestors, f)\nprint(\" Ancestor graph saved.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def extract_uniprot_id(header):\n    # Extract between first pair of pipes ‚Üí sp|XXXX| or tr|XXXX|\n    m = re.match(r\"^[a-z]{2}\\|([^|]+)\\|\", header)\n    if m:\n        return m.group(1)\n    return header  # fallback\n\n# Build mapping: extract UniProt ID for every FASTA protein\ntrain_proteins_raw = list(train_seqs.keys())\ntrain_proteins = [extract_uniprot_id(h) for h in train_proteins_raw]\n\nprot_map = {pid: i for i, pid in enumerate(train_proteins)}\n\n# Filter annotation proteins to the ones that appear in FASTA\nvalid_annots = train_terms_df[train_terms_df[\"EntryID\"].isin(prot_map)]\n\nprint(\"Proteins in FASTA:\", len(train_seqs))\nprint(\"Proteins in annotations:\", len(train_terms_df))\nprint(\"Overlap:\", len(valid_annots))\n\nused_terms = set(train_terms_df[\"term\"].unique())\n\nterms_MF = [t for t in used_terms if t in go_terms and go_terms[t]['namespace'] == 'molecular_function']\nterms_BP = [t for t in used_terms if t in go_terms and go_terms[t]['namespace'] == 'biological_process']\nterms_CC = [t for t in used_terms if t in go_terms and go_terms[t]['namespace'] == 'cellular_component']\n\n\nterm_maps = {\n    'MF': {str(i): t for i, t in enumerate(terms_MF)},\n    'BP': {str(i): t for i, t in enumerate(terms_BP)},\n    'CC': {str(i): t for i, t in enumerate(terms_CC)}\n}\n\nwith open(os.path.join(OUTPUT_LABELS_DIR, \"maps.json\"), \"w\") as f:\n    json.dump(term_maps, f)\n\nfor ns, terms_list in [('MF', terms_MF), ('BP', terms_BP), ('CC', terms_CC)]:\n\n    go_to_idx = {t: i for i, t in enumerate(terms_list)}\n\n    df_ns = valid_annots[valid_annots[\"term\"].isin(terms_list)]\n    \n    p_idx = df_ns[\"EntryID\"].map(prot_map).values\n    t_idx = df_ns[\"term\"].map(go_to_idx).values\n\n    mat = sparse.coo_matrix(\n        (np.ones(len(p_idx)), (p_idx, t_idx)),\n        shape=(len(train_proteins), len(terms_list)),\n        dtype=np.int8\n    ).tocsr()\n\n    sparse.save_npz(os.path.join(OUTPUT_LABELS_DIR, f\"labels_{ns}.npz\"), mat)\n\n    print(f\"{ns}: proteins={mat.shape[0]}, terms={mat.shape[1]}, nonzero={mat.nnz}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#cheking the .npz files\nfor ns in [\"MF\", \"BP\", \"CC\"]:\n    mat = sparse.load_npz(os.path.join(OUTPUT_LABELS_DIR, f\"labels_{ns}.npz\"))\n    print(ns, \"nonzero:\", mat.count_nonzero(), \"shape:\", mat.shape)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"##  Part 3: Feature Engineering (Embeddings)\nHere I will now extract rich features from protein sequences using **three** different models. This is the secret sauce! \nWe define a generic embedding function to reuse for all models.","metadata":{}},{"cell_type":"code","source":"def embed_sequences(model_name, seq_dict, batch_size=16, max_len=1024):\n\n    # FORCE GPU ONLY\n    if not torch.cuda.is_available():\n        raise SystemExit(\" GPU NOT FOUND ‚Äî Cannot embed on CPU. Aborting as requested.\")\n\n    device = torch.device(\"cuda\")\n    print(\" FORCING GPU:\", device)\n\n    name = model_name.lower()\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    model = AutoModel.from_pretrained(\n            model_name,\n            torch_dtype=torch.float16\n        )\n\n    model = model.to(device)\n    model.eval()\n\n    seqs = list(seq_dict.values())\n    ids = list(seq_dict.keys())\n    embeddings = []\n\n    for i in tqdm(range(0, len(seqs), batch_size)):\n        batch = seqs[i:i + batch_size]\n\n        # TOKENIZATION RULES\n        if \"t5\" in name:\n            batch_tok = [\" \".join(list(s)) for s in batch]\n        elif \"bert\" in name:\n            batch_tok = [\" \".join(list(s)) for s in batch]\n        else:\n            batch_tok = batch   \n\n        inputs = tokenizer(\n            batch_tok,\n            return_tensors=\"pt\",\n            padding=True,\n            truncation=True,\n            max_length=max_len\n        ).to(device)\n\n        with torch.no_grad():\n            with torch.cuda.amp.autocast(dtype=torch.float16):\n\n                outputs = model(**inputs)\n                hidden = outputs.last_hidden_state   \n                mask = inputs[\"attention_mask\"].unsqueeze(-1).float()\n\n                # mean pooling\n                emb = (hidden * mask).sum(dim=1) / mask.sum(dim=1).clamp(min=1e-9)\n\n        # move to CPU only at the last moment\n        embeddings.append(emb.float().cpu().numpy())\n\n        torch.cuda.empty_cache()\n        gc.collect()\n\n    return np.concatenate(embeddings), ids\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Note: I have done the embedding locally so you can check the dataset**","metadata":{}},{"cell_type":"code","source":"# emb_train_bert, _ = embed_sequences(\"Rostlab/prot_bert\", train_seqs, batch_size=128)\n# np.save(\"train_protbert.npy\", emb_train_bert)\n# del emb_train_bert\n\n# emb_test_bert, test_ids = embed_sequences(\"Rostlab/prot_bert\", test_seqs, batch_size=128)\n# np.save(\"test_protbert.npy\", emb_test_bert)\n# np.save(\"test_ids.npy\", test_ids)\n# del emb_test_bert\nprint(\"Protbert embeddings saved\") ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# emb_train_esm, _ = embed_sequences(\"facebook/esm2_t30_150M_UR50D\", train_seqs, batch_size=8)\n# np.save(\"train_esm2.npy\", emb_train_esm)\n# del emb_train_esm\n\n# emb_test_esm, _ = embed_sequences(\"facebook/esm2_t30_150M_UR50D\", test_seqs, batch_size=8)\n# np.save(\"test_esm2.npy\", emb_test_esm)\n# del emb_test_esm\nprint(\"ESM-2(150M) embeddings saved\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**I am using a different embedding function for Prot5 cause I was finding difficulty to create a single unifies one for Prott5**","metadata":{}},{"cell_type":"markdown","source":"**We freeze ProtT5 and generate contextual protein embeddings, then train lightweight downstream heads.**","metadata":{}},{"cell_type":"code","source":"# model_name = \"Rostlab/prot_t5_xl_uniref50\"\n# tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False, do_lower_case=False)\n\n# def preprocess_for_t5(seq):\n#     return \" \".join(list(seq))\n\n# def tokenize_all(seqs, max_len=1024):\n#     tok_inputs = []\n#     for s in tqdm(seqs, desc=\"Pre-tokenizing\"):\n#         t = tokenizer(\n#             preprocess_for_t5(s),\n#             truncation=True,\n#             max_length=max_len,\n#             padding=\"max_length\",   # <- CRITICAL for speed\n#             return_tensors=\"np\"\n#         )\n#         tok_inputs.append((t[\"input_ids\"][0], t[\"attention_mask\"][0]))\n#     return np.array([x[0] for x in tok_inputs]), np.array([x[1] for x in tok_inputs])\n\n# train_ids_np, train_mask_np = tokenize_all(list(train_seqs.values()))\n# np.save(\"train_ids.npy\", train_ids_np)\n# np.save(\"train_mask.npy\", train_mask_np)\nprint(\"Saved: train_ids.npy, train_mask.npy\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# def tokenize_all(seqs, max_len=1024):\n#     tok_inputs = []\n#     for s in tqdm(seqs, desc=\"Pre-tokenizing\"):\n#         t = tokenizer(\n#             preprocess_for_t5(s),\n#             truncation=True,\n#             max_length=max_len,\n#             padding=\"max_length\",\n#             return_tensors=\"np\"\n#         )\n#         tok_inputs.append((t[\"input_ids\"][0], t[\"attention_mask\"][0]))\n#     return np.array([x[0] for x in tok_inputs]), np.array([x[1] for x in tok_inputs])\n\n# test_ids_np, test_mask_np = tokenize_all(list(test_seqs.values()))\n\n# np.save(\"test_ids.npy\", test_ids_np)\n# np.save(\"test_mask.npy\", test_mask_np)\n\nprint(\"Saved: test_ids.npy, test_mask.npy\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_ids = np.load(\"/kaggle/input/embeddings-for-cafa/train_ids_T5.npy\")\ntrain_mask = np.load(\"/kaggle/input/embeddings-for-cafa/train_mask.npy\")\ntest_ids = np.load(\"/kaggle/input/embeddings-for-cafa/test_ids_T5.npy\")\ntest_mask = np.load(\"/kaggle/input/embeddings-for-cafa/test_mask.npy\")\nprint('loaded the npy files necessary to embed T5')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def prot_t5_embed(\n    model_name: str,\n    ids_np: np.ndarray,\n    mask_np: np.ndarray,\n    batch_size: int = 32,\n    layer_idx: int = 12,\n):\n\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    # Load model\n    model = T5EncoderModel.from_pretrained(\n        model_name,\n        torch_dtype=torch.float16 if device.type == \"cuda\" else torch.float32\n    ).to(device)\n\n    model.eval()\n\n    N = ids_np.shape[0]\n    hidden_dim = model.config.d_model\n\n    # Preallocate output\n    embeddings = np.zeros((N, hidden_dim), dtype=np.float32)\n\n    for start in tqdm(range(0, N, batch_size), desc=\"ProtT5 Embedding\"):\n        end = min(start + batch_size, N)\n\n        ids = torch.tensor(ids_np[start:end], device=device)\n        mask = torch.tensor(mask_np[start:end], device=device)\n\n        with torch.no_grad():\n            with torch.cuda.amp.autocast(enabled=(device.type == \"cuda\")):\n                outputs = model(\n                    input_ids=ids,\n                    attention_mask=mask,\n                    output_hidden_states=True,\n                    return_dict=True\n                )\n\n                # Extract chosen layer\n                h = outputs.hidden_states[layer_idx]   # (B, L, 1024)\n\n                # Masked mean pooling\n                mask_f = mask.unsqueeze(-1)\n                pooled = (h * mask_f).sum(1) / mask_f.sum(1).clamp(min=1e-9)\n\n        embeddings[start:end] = pooled.float().cpu().numpy()\n\n        del ids, mask, h, pooled\n\n    return embeddings\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# emb_t5_train = prot_t5_embed(\n#     \"Rostlab/prot_t5_xl_uniref50\",\n#     train_ids, train_mask,\n#     batch_size=4\n# )\n\n# np.save(\"/kaggle/working/train_prott5.npy\", emb_t5_train)\n# del emb_t5_train\n\nprint(\"Training prott5 completed\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**test chunks were large so that's why I am chunking and combining them**","metadata":{}},{"cell_type":"code","source":"# MODEL_NAME = \"Rostlab/prot_t5_xl_uniref50\"\n# MAX_LEN = 512\n# LAYER_IDX = 12\n# EMB_DIM = 1024\n\n# MICRO_BATCH = 4                 # per TPU core (SAFE)\n# NUM_CORES = len(xm.get_xla_supported_devices())  # ‚úÖ ALWAYS WORKS\n# GLOBAL_BATCH = MICRO_BATCH * NUM_CORES            # 32\n\n# CHUNK_SIZE = 4096               # proteins per chunk (~2‚Äì3 min)\n# OUT_DIR = \"/kaggle/working/prot_t5_test_chunks\"\n# os.makedirs(OUT_DIR, exist_ok=True)\n\n# print(\" TPU devices:\", xm.get_xla_supported_devices())\n# print(\" TPU cores:\", NUM_CORES)\n# print(\" Global batch:\", GLOBAL_BATCH)\n\n# # ======================================================\n# # DATA\n# # ======================================================\n# # test_ids, test_mask must already exist\n# N = test_ids.shape[0]\n# print(\"Total test proteins:\", N)\n\n# # ======================================================\n# # MODEL (TPU)\n# # ======================================================\n# device = xm.xla_device()\n\n# model = T5EncoderModel.from_pretrained(\n#     MODEL_NAME,\n#     torch_dtype=torch.bfloat16\n# ).to(device)\n\n# model.eval()\n\n# # ======================================================\n# # RESUME LOGIC\n# # ======================================================\n# def get_next_chunk_id(out_dir):\n#     files = [f for f in os.listdir(out_dir) if f.startswith(\"chunk_\")]\n#     if not files:\n#         return 0\n#     ids = sorted(int(f.split(\"_\")[1].split(\".\")[0]) for f in files)\n#     return ids[-1] + 1\n\n# start_chunk = get_next_chunk_id(OUT_DIR)\n# print(\" Resuming from chunk:\", start_chunk)\n\n# # ======================================================\n# # MAIN LOOP\n# # ======================================================\n# start_time = time.time()\n# num_chunks = math.ceil(N / CHUNK_SIZE)\n\n# for chunk_id in range(start_chunk, num_chunks):\n#     c_start = chunk_id * CHUNK_SIZE\n#     c_end = min(N, c_start + CHUNK_SIZE)\n#     cur_N = c_end - c_start\n\n#     print(f\"\\n Chunk {chunk_id} | proteins {c_start}:{c_end}\")\n\n#     out_chunk = np.zeros((cur_N, EMB_DIM), dtype=np.float32)\n#     write_ptr = 0\n\n#     for i in tqdm(\n#         range(c_start, c_end, GLOBAL_BATCH),\n#         desc=f\"TPU Chunk {chunk_id}\",\n#         leave=False\n#     ):\n#         j = min(i + GLOBAL_BATCH, c_end)\n#         bs = j - i\n\n#         ids = test_ids[i:j, :MAX_LEN]\n#         mask = test_mask[i:j, :MAX_LEN]\n\n#         # Pad to GLOBAL_BATCH\n#         if bs < GLOBAL_BATCH:\n#             pad = GLOBAL_BATCH - bs\n#             ids = np.pad(ids, ((0, pad), (0, 0)))\n#             mask = np.pad(mask, ((0, pad), (0, 0)))\n\n#         ids_t = torch.tensor(ids, device=device)\n#         mask_t = torch.tensor(mask, device=device)\n\n#         with torch.no_grad():\n#             out = model(\n#                 input_ids=ids_t,\n#                 attention_mask=mask_t,\n#                 output_hidden_states=True,\n#                 return_dict=True\n#             )\n\n#             h = out.hidden_states[LAYER_IDX]     # (B, L, 1024)\n#             mask_f = mask_t.unsqueeze(-1).to(h.dtype)\n#             pooled = (h * mask_f).sum(1) / mask_f.sum(1).clamp(min=1e-9)\n\n#         pooled = pooled[:bs].float().cpu().numpy()\n#         out_chunk[write_ptr:write_ptr + bs] = pooled\n#         write_ptr += bs\n\n#         xm.mark_step()   # TPU sync\n#         del ids_t, mask_t, h, pooled\n#         gc.collect()\n\n#     # SAVE CHUNK\n#     chunk_path = os.path.join(OUT_DIR, f\"chunk_{chunk_id:03d}.npy\")\n#     np.save(chunk_path, out_chunk)\n#     print(f\" Saved {chunk_path}\")\n\n# elapsed = (time.time() - start_time) / 60\nprint(f\"\\n DONE with ProtT5 Embeddings\")\n# print(f\" Total time: {elapsed:.1f} minutes\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# files = sorted(glob.glob(\"/kaggle/working/prot_t5_test_chunks/chunk_*.npy\"))\n# X = np.concatenate([np.load(f) for f in files], axis=0)\n\n# np.save(\"/kaggle/working/prot_t5_test.npy\", X)\nprint(\" Final test embeddings saved\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"##  Part 4: Training the Single-Head Classifier\n\nWe will train a separate **Single-Layer Perceptron (MLP)** for each embedding type.  SLP would have three heads:  Molecular Function (MF), Biological Process (BP), Cellular Component (CC).\n\nI tried using Multi-Head classifier but the problem is I was constantly getting Memoryerror that's why swithced to single head","metadata":{}},{"cell_type":"code","source":"BASE_EMB = \"/kaggle/input/embeddings-for-cafa/\"         # ProtBERT & ESM2 embedding files\nBASE_LABELS = \"/kaggle/working/outputs_labels/\"  # labels_MF.npz, labels_BP.npz, labels_CC.npz\n\nlabels_MF = sparse.load_npz(BASE_LABELS + \"labels_MF.npz\")\nlabels_BP = sparse.load_npz(BASE_LABELS + \"labels_BP.npz\")x\nlabels_CC = sparse.load_npz(BASE_LABELS + \"labels_CC.npz\")\n\nlabels_by_ns = {\n    \"MF\": labels_MF,\n    \"BP\": labels_BP,\n    \"CC\": labels_CC\n}\n\nout_dims = {\n    ns: mat.shape[1] for ns, mat in labels_by_ns.items()\n}\n\nprint(\" Loaded labels. Output dimensions:\", out_dims)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# All protein indices\nall_ids = np.arange(labels_MF.shape[0])\n\n# 10% validation split\ntrain_idx, val_idx = train_test_split(all_ids, test_size=0.1, random_state=42)\n\ndef split_labels(mat):\n    return mat[train_idx], mat[val_idx]\n\nlabels_train = {}\nlabels_val = {}\n\n# Split MF/BP/CC label matrices\nfor ns, mat in labels_by_ns.items():\n    labels_train[ns], labels_val[ns] = split_labels(mat)\n\n# Load embeddings\ntrain_emb_prot = np.load(BASE_EMB + \"train_protbert.npy\", mmap_mode=\"r\")\ntrain_emb_esm  = np.load(BASE_EMB + \"train_esm2.npy\", mmap_mode=\"r\")\n\n# Split embeddings\nprot_train = train_emb_prot[train_idx]\nprot_val   = train_emb_prot[val_idx]\n\nesm_train  = train_emb_esm[train_idx]\nesm_val    = train_emb_esm[val_idx]\n\nprint(\"Train/Val Split Complete\")\nprint(\"ProtBERT train:\", prot_train.shape, \"val:\", prot_val.shape)\nprint(\"ESM2 train:\", esm_train.shape, \"val:\", esm_val.shape)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n\n# confirming device detection\nUSING_TPU = False\ntry:\n    import torch_xla.core.xla_model as xm\n    device_tpu = xm.xla_device()\n    USING_TPU = True\nexcept Exception:\n    device_tpu = None\n\nif torch.cuda.is_available():\n    device = torch.device(\"cuda\")\n    print(\" GPU detected. Executing on CUDA.\")\nelif USING_TPU:\n    device = device_tpu\n    print(\" TPU detected. Executing on TPU.\")\nelse:\n    device = torch.device(\"cpu\")\n    print(\" CPU fallback:\", device)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class SingleHeadMLP(nn.Module):\n    def __init__(self, input_dim, proj_dim, out_dim):\n        super().__init__()\n\n        hidden_dim = proj_dim\n\n        self.ln_in = nn.LayerNorm(input_dim)\n\n        self.block1 = nn.Sequential(\n            nn.Linear(input_dim, hidden_dim),\n            nn.LayerNorm(hidden_dim),\n            nn.GELU(),\n            nn.Dropout(0.3),\n        )\n\n        self.block2 = nn.Sequential(\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.LayerNorm(hidden_dim),\n            nn.GELU(),\n            nn.Dropout(0.3),\n        )\n\n        self.out_linear = nn.Linear(hidden_dim, out_dim)\n\n    def forward(self, x):\n        x = self.ln_in(x)\n\n        h1 = self.block1(x)\n        h2 = self.block2(h1)\n\n        return self.out_linear(h2 + h1)   \n        \nclass ProteinDatasetNS(Dataset):\n    def __init__(self, emb_memmap, label_mat):\n        self.emb = emb_memmap\n        self.labels_by_ns = label_mat\n\n    def __len__(self):\n        return self.emb.shape[0]\n\n    def __getitem__(self, idx):\n        emb = torch.tensor(self.emb[idx], dtype=torch.float32)\n        lab = torch.tensor(self.labels_by_ns[idx].toarray().ravel(), dtype=torch.float32)\n        return emb, lab","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train_ns_only(model_name, ns, train_emb_path, label_mat,\n                  proj_dim=768, batch_size=16, epochs=12,\n                  train_emb_tensor=None, val_emb=None, val_labels=None):\n\n    print(f\"\\n[{model_name} | {ns}] TRAINING...\")\n\n    if train_emb_tensor is not None:\n        train_emb = train_emb_tensor  \n    else:\n        train_emb = np.load(train_emb_path, mmap_mode=\"r\")\n\n    input_dim = train_emb.shape[1]\n    out_dim   = label_mat.shape[1]\n\n    dataset = ProteinDatasetNS(train_emb, label_mat)\n    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n\n    model = SingleHeadMLP(input_dim, proj_dim, out_dim).to(device)\n    optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, weight_decay=1e-4)\n    \n    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n        optimizer,\n        T_max=epochs,\n        eta_min=3e-5  \n    )\n\n    label_counts = np.array(label_mat.sum(axis=0)).ravel()\n    neg_counts   = (label_mat.shape[0] - label_counts)\n\n    pos_weight_np = neg_counts / (label_counts + 1e-6)\n\n    pos_weight_np = np.clip(pos_weight_np, 1.0, 50.0)  # you can try 20.0‚Äì50.0 range\n    \n    pos_weight = torch.tensor(pos_weight_np, dtype=torch.float32).to(device)\n    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n\n    # Mixed precision\n    use_amp = (torch.cuda.is_available() and not USING_TPU)\n    scaler = torch.amp.GradScaler(enabled=use_amp)\n\n    for epoch in range(epochs):\n        model.train()\n        total_loss = 0.0\n\n        for emb, lab in tqdm(dataloader, desc=f\"{model_name} {ns} Epoch {epoch+1}\", leave=False):\n            emb, lab = emb.to(device), lab.to(device)\n            optimizer.zero_grad(set_to_none=True)\n\n            if USE_TPU:\n                out = model(emb)\n                loss = criterion(out, lab)\n                loss.backward()\n                xm.optimizer_step(optimizer)\n                xm.mark_step()\n\n            else:\n                if use_amp:\n                    with torch.amp.autocast(\"cuda\", dtype=torch.float16):\n                        out = model(emb)\n                        loss = criterion(out, lab)\n                    scaler.scale(loss).backward()\n                    scaler.step(optimizer)\n                    scaler.update()\n                else:\n                    out = model(emb)\n                    loss = criterion(out, lab)\n                    loss.backward()\n                    optimizer.step()\n\n            total_loss += loss.item()\n\n        avg_loss = total_loss / max(1, len(dataloader))\n        print(f\"[{model_name} | {ns}] Epoch {epoch+1}/{epochs} - Loss = {avg_loss:.4f}\")\n    \n        scheduler.step()\n\n    # Cleanup\n    del dataset, dataloader, train_emb\n    gc.collect()\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n\n    model.eval()\n    print(f\"[{model_name} | {ns}] TRAINING COMPLETE.\")\n\n    DIAGNOSE = True\n    if DIAGNOSE:\n        print(f\"\\n[{model_name} | {ns}] Running inference diagnostics...\")\n\n        if input_dim == 1024:\n            test_path = BASE_EMB + \"test_protbert.npy\"\n        elif input_dim == 640:\n            test_path = BASE_EMB + \"test_esm2.npy\"\n        else:\n            raise ValueError(f\"Unknown embedding dim: {input_dim}\")\n\n        test_emb_small = np.load(test_path, mmap_mode=\"r\")[:4]\n        test_emb_small = torch.tensor(test_emb_small, dtype=torch.float32).to(device)\n\n        with torch.no_grad():\n            raw_logits = model(test_emb_small)\n            probs = torch.sigmoid(raw_logits)\n\n        print(\"\\nSample probabilities (first protein, first 20 terms):\")\n        print(probs[0][:20].cpu().numpy())\n\n        print(\"\\nMax prob:\", probs.max().item())\n        print(\"Min prob:\", probs.min().item())\n        print(\"Mean prob:\", probs.mean().item())\n\n        if probs.max().item() < 0.001:\n            print(\"\\nWARNING: Model predictions are EXTREMELY SMALL ‚Äî empty submission risk.\")\n        else:\n            print(\"\\nDiagnostics OK ‚Äî model produces real signals.\")\n\n        del test_emb_small, raw_logits, probs\n        gc.collect()\n\n    return model\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"labels_MF = labels_by_ns[\"MF\"]\nlabels_BP = labels_by_ns[\"BP\"]\nlabels_CC = labels_by_ns[\"CC\"]\n\nnamespaces = [\"MF\", \"BP\", \"CC\"]\n\nprotbert_models = {}\nesm2_models = {}\n\nfor ns in namespaces:\n    print(f\"\\n========== NAMESPACE: {ns} ==========\")\n\n    # Train ProtBERT \n    protbert_models[ns] = train_ns_only(\n        model_name=\"ProtBERT\",\n        ns=ns,\n        train_emb_path=os.path.join(BASE_EMB, \"train_protbert.npy\"),\n        label_mat=labels_train[ns],\n        train_emb_tensor=prot_train,     \n        val_emb=prot_val,\n        val_labels=labels_val[ns]\n    )\n\n\n\n    # Train ESM2 \n    esm2_models[ns] = train_ns_only(\n        model_name=\"ESM2-150M\",\n        ns=ns,\n        train_emb_path=os.path.join(BASE_EMB, \"train_esm2.npy\"),\n        label_mat=labels_train[ns],\n        train_emb_tensor=esm_train,       \n        val_emb=esm_val,\n        val_labels=labels_val[ns]\n    )\n\n\n\nprint(\"\\nAll ProtBERT and ESM2 models trained\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"##  Part 5: Submission\n\nAfter completing training for all namespaces (MF, BP, CC) with both the ProtBERT and ESM2-150M models, we generate predictions for the full CAFA-6 test set.\nInstead of saving massive .npy files (which corrupt easily and exceed Kaggle‚Äôs storage limits), we execute a streamed ensemble pipeline","metadata":{}},{"cell_type":"code","source":"# Load mappings and ancestors (unchanged)\nwith open(os.path.join(OUTPUT_LABELS_DIR, \"maps.json\")) as f:\n    maps = json.load(f)\n\nwith open(os.path.join(OUTPUT_LABELS_DIR, \"ancestors.json\")) as f:\n    ancestors = json.load(f)\n\nIMPORTED_EMBEDDINGS_DIR = BASE_EMB\n\ntest_ids = np.load(os.path.join(IMPORTED_EMBEDDINGS_DIR, \"test_ids.npy\"))\ntest_prot = np.load(os.path.join(IMPORTED_EMBEDDINGS_DIR, \"test_protbert.npy\"), mmap_mode=\"r\")\ntest_esm  = np.load(os.path.join(IMPORTED_EMBEDDINGS_DIR, \"test_esm2.npy\"), mmap_mode=\"r\")\n\nN_test = test_prot.shape[0]\nassert len(test_ids) == N_test, \"Mismatch between test_ids and test_prot size.\"\n\nprint(f\"Test proteins: {N_test}\")\nprint(\"Starting streamed ensemble + submission generation...\")\n\nensemble_weights = {\n    \"MF\": (0.4, 0.6),  \n    \"BP\": (0.3, 0.7),\n    \"CC\": (0.5, 0.5)\n}\n\ntemperatures = {\n    \"MF\": 0.9,\n    \"BP\": 0.8,\n    \"CC\": 1.0\n}\n\nthresholds = {\n    \"MF\": 1e-3,\n    \"BP\": 2e-3,\n    \"CC\": 5e-4\n}\n\nmaps_idx_to_go = {}\nfor ns in namespaces:\n    D = len(maps[ns])\n    arr = np.empty(D, dtype=object)\n    for i in range(D):\n        arr[i] = maps[ns][str(i)]\n    maps_idx_to_go[ns] = arr\n\nancestors_fast = ancestors\n\nTOPK_RAW = 3000\n\nCHUNK_SIZE = 512\nsubmission_path = os.path.join(WORKING_DIR, \"submission.tsv\")\n\nwith open(submission_path, \"w\") as fout:\n    for start in tqdm(range(0, N_test, CHUNK_SIZE), desc=\"Batches\"):\n        end = min(N_test, start + CHUNK_SIZE)\n\n        # Load embeddings for this batch\n        prot_batch_np = test_prot[start:end]\n        esm_batch_np  = test_esm[start:end]\n\n        prot_batch = torch.tensor(prot_batch_np, dtype=torch.float32).to(device)\n        esm_batch  = torch.tensor(esm_batch_np, dtype=torch.float32).to(device)\n\n        ns_scores = {}\n\n        for ns in namespaces:\n            w_pb, w_es = ensemble_weights[ns]\n            T = temperatures[ns]\n\n            with torch.no_grad():\n                pb_logits = protbert_models[ns](prot_batch)\n                es_logits = esm2_models[ns](esm_batch)\n\n                logits = (w_pb * pb_logits) + (w_es * es_logits)\n                logits = logits / T\n\n                scores = torch.sigmoid(logits).cpu().numpy()\n\n            ns_scores[ns] = scores\n\n        batch_ids = test_ids[start:end]\n        chunk_lines = [] \n\n        for local_idx, pid in enumerate(batch_ids):\n            protein_lines = []\n\n            for ns in namespaces:\n                scores = ns_scores[ns][local_idx]\n                th = thresholds[ns]\n\n                idxs = np.where(scores > th)[0]\n                if idxs.size == 0:\n                    continue\n\n                if idxs.size > TOPK_RAW:\n                    topk_idx = np.argpartition(scores[idxs], -TOPK_RAW)[-TOPK_RAW:]\n                    idxs = idxs[topk_idx]\n\n                go_terms = maps_idx_to_go[ns][idxs]\n                vals = scores[idxs]\n\n                term_scores = {go_id: float(v) for go_id, v in zip(go_terms, vals)}\n\n                prop = dict(term_scores)\n                for go_id, score in term_scores.items():\n                    if go_id in ancestors_fast:\n                        for anc in ancestors_fast[go_id]:\n                            if anc not in prop or score > prop[anc]:\n                                prop[anc] = score\n\n                protein_lines.extend(prop.items())\n\n            if protein_lines:\n                protein_lines.sort(key=lambda x: x[1], reverse=True)\n                for go_id, score in protein_lines[:1500]:\n                    chunk_lines.append(f\"{pid}\\t{go_id}\\t{score:.3f}\\n\")\n\n\n        if chunk_lines:\n            fout.writelines(chunk_lines)\n\n        del prot_batch, esm_batch, prot_batch_np, esm_batch_np, ns_scores, chunk_lines\n        gc.collect()\n        if torch.cuda.is_available():\n            torch.cuda.empty_cache()\n\nprint(f\"Submission written to: {submission_path}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Paths\nWORK_DIR = \"/kaggle/working/\"\nsub_path = os.path.join(WORK_DIR, \"submission.tsv\")\nids_path = \"/kaggle/input/embeddings-for-cafa/test_ids.npy\"\nmaps_path = os.path.join(WORK_DIR, \"outputs_labels/maps.json\")\n\nprint(\"üîç Loading data...\")\n\n# Load submissi\ntry:\n    df = pd.read_csv(sub_path, sep=\"\\t\", header=None, names=[\"protein\", \"goterm\", \"score\"])\nexcept:\n    raise ValueError(\"‚ùå FAILED TO READ submission.tsv. Check file format.\")\n\n# Load test IDs\ntest_ids = np.load(ids_path)\n\n# Load GO maps\nwith open(maps_path) as f:\n    maps = json.load(f)\n\n# Build GO term set\nall_go_terms = set()\nfor ns in [\"MF\", \"BP\", \"CC\"]:\n    for k, v in maps[ns].items():\n        all_go_terms.add(v)\n\nprint(\"‚úÖ Loaded submission, test IDs, and GO term maps.\")\n\n\n# ------------------------\n# 1. EMPTY FILE CHECK\n# ------------------------\nprint(\"\\n=== EMPTY FILE CHECK ===\")\nif df.shape[0] == 0:\n    print(\"‚ùå ERROR: submission.tsv is EMPTY.\")\n    print(\"Possible causes:\")\n    print(\"- You used an incorrect final_preds path.\")\n    print(\"- Your submission loop never appended predictions.\")\n    print(\"- Maps.json or ancestors.json mismatched.\")\n    raise SystemExit()\n\nprint(f\"‚úÖ submission.tsv contains {df.shape[0]} rows.\")\n\n\n# ------------------------\n# 2. BASIC STRUCTURE CHECK\n# ------------------------\nprint(\"\\n=== BASIC STRUCTURE CHECK ===\")\n\nif df.shape[1] != 3:\n    print(\"‚ùå ERROR: submission must have 3 columns: protein, goterm, score\")\n    print(\"You have:\", df.shape[1])\n    raise SystemExit()\n\nprint(\"‚úÖ Correct number of columns.\")\n\n\n# ------------------------\n# 3. CHECK IF ALL PROTEINS APPEAR\n# ------------------------\nprint(\"\\n=== PROTEIN COUNT CHECK ===\")\n\nunique_proteins = df[\"protein\"].nunique()\nexpected = len(test_ids)\n\nprint(\"Proteins in submission:\", unique_proteins)\nprint(\"Proteins expected:\", expected)\n\nif unique_proteins == 0:\n    print(\"‚ùå ERROR: No proteins found in submission.\")\n    raise SystemExit()\n\nif unique_proteins < expected:\n    print(\"‚ö†Ô∏è WARNING: Missing proteins from submission.\")\nelse:\n    print(\"‚úÖ All proteins present (or extra rows exist).\")\n\n\n# ------------------------\n# 4. SCORE VALIDITY CHECK\n# ------------------------\nprint(\"\\n=== SCORE CHECK ===\")\n\nbad_scores = df[~df[\"score\"].between(0, 1)]\n\nif len(bad_scores) > 0:\n    print(f\"‚ùå ERROR: {len(bad_scores)} invalid scores found.\")\nelse:\n    print(\"‚úÖ All scores are between 0 and 1.\")\n\n\n# ------------------------\n# 5. GO TERM VALIDITY CHECK\n# ------------------------\nprint(\"\\n=== GO TERM VALIDATION ===\")\n\ninvalid_go = df[~df[\"goterm\"].isin(all_go_terms)]\n\nif len(invalid_go) > 0:\n    print(f\"‚ö†Ô∏è WARNING: {len(invalid_go)} GO terms not found in maps.json.\")\nelse:\n    print(\"‚úÖ All GO terms are known and valid.\")\n\n\n# ------------------------\n# 6. DUPLICATE CHECK\n# ------------------------\nprint(\"\\n=== DUPLICATE PER-PROTEIN CHECK ===\")\n\ndupes = df[df.duplicated(subset=[\"protein\", \"goterm\"], keep=False)]\nif len(dupes) > 0:\n    print(f\"‚ö†Ô∏è WARNING: Found {len(dupes)} duplicate GO annotations.\")\nelse:\n    print(\"‚úÖ No duplicates detected.\")\n\n\n# ------------------------\n# 7. TOP 1500 LIMIT CHECK\n# ------------------------\nprint(\"\\n=== PREDICTION LIMIT CHECK (1500 per protein) ===\")\n\ncounts = df.groupby(\"protein\").size()\nover = counts[counts > 1500]\n\nif len(over) > 0:\n    print(f\"‚ùå ERROR: {len(over)} proteins exceed 1500 GO terms.\")\nelse:\n    print(\"‚úÖ All proteins obey the 1500-term rule.\")\n\n\n# ------------------------\n# 8. SAFE SAMPLE PREVIEW\n# ------------------------\nprint(\"\\n=== SAMPLE PREVIEW ===\")\n\nprint(df.head(10))\n\nfirst_protein = df[\"protein\"].iloc[0]   # SAFE now because df is non-empty\nprint(f\"\\nSample GO terms for protein {first_protein}:\")\nprint(df[df[\"protein\"] == first_protein].head(20))\n\n\nprint(\"\\n VALIDATION COMPLETE ‚Äî FILE IS STRUCTURALLY SOUND.\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}